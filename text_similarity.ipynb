{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text_similarity.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jr9K0W0Gjsrt"
      },
      "source": [
        "# n-gram 모델\n",
        "- n-gram 모델은 자연어 처리, 정보 검색 등에서 활용이 되는 시퀀스 데이터 표현 방식입니다. \n",
        "- ngram에서 n은 연속된 단어의 개수를 의미합니다. \n",
        "\n",
        "* n=1 : 1-gram(unigram)\n",
        "* n=2 : 2-gram(bigram)\n",
        "* n=3 : 3-gram(trigram)\n",
        "* n=4 : 4-gram"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CVk-BDZai5Gr",
        "outputId": "5466161f-4410-4013-873d-6f73dda7f936"
      },
      "source": [
        "!pip install konlpy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: konlpy in /usr/local/lib/python3.7/dist-packages (0.5.2)\n",
            "Requirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (3.10.0)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.2.1)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.19.5)\n",
            "Requirement already satisfied: beautifulsoup4==4.6.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.6.0)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.7/dist-packages (from konlpy) (0.4.4)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2020.12.5)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwAvtBgNiyCh"
      },
      "source": [
        "from konlpy.tag import Okt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0V1AF7zix_Z"
      },
      "source": [
        "# 어절 단위 n-gram\n",
        "def word_ngram(bow, num_gram):\n",
        "    text = tuple(bow)\n",
        "    ngrams = [text[x:x + num_gram] for x in range(0, len(text))]\n",
        "    return tuple(ngrams)\n",
        "\n",
        "\n",
        "# 음절 n-gram 분석\n",
        "def phoneme_ngram(bow, num_gram):\n",
        "    sentence = ' '.join(bow)\n",
        "    text = tuple(sentence)\n",
        "    slen = len(text)\n",
        "    ngrams = [text[x:x + num_gram] for x in range(0, slen)]\n",
        "    return ngrams\n",
        "\n",
        "\n",
        "# 유사도 계산\n",
        "def similarity(doc1, doc2):\n",
        "    cnt = 0\n",
        "    for token in doc1:\n",
        "        if token in doc2:\n",
        "            cnt = cnt + 1\n",
        "\n",
        "    return cnt/len(doc1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61U2DQVqix6J"
      },
      "source": [
        "sentence1 = '6월에 뉴턴은 선생님의 제안으로 트리니티에 입학하였다'\n",
        "sentence2 = '6월에 뉴턴은 선생님의 제안으로 대학교에 입학하였다'\n",
        "sentence3 = '나는 맛잇는 밥을 뉴턴 선생님과 함께 먹었습니다.'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDkO2yVyixw1",
        "outputId": "8f3b0f69-1736-41e2-f587-8461e18c53b8"
      },
      "source": [
        "okt = Okt()\n",
        "bow1 = okt.nouns(sentence1)\n",
        "bow2 = okt.nouns(sentence2)\n",
        "bow3 = okt.nouns(sentence3)\n",
        "\n",
        "doc1 = word_ngram(bow1, 2)\n",
        "doc2 = word_ngram(bow2, 2)\n",
        "doc3 = word_ngram(bow3, 2)\n",
        "\n",
        "print(doc1)\n",
        "print(doc2)\n",
        "print(doc3)\n",
        "\n",
        "r1 = similarity(doc1, doc2)\n",
        "r2 = similarity(doc3, doc1)\n",
        "print(r1)\n",
        "print(r2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(('뉴턴', '선생님'), ('선생님', '제안'), ('제안', '트리니티'), ('트리니티', '입학'), ('입학',))\n",
            "(('뉴턴', '선생님'), ('선생님', '제안'), ('제안', '대학교'), ('대학교', '입학'), ('입학',))\n",
            "(('나', '맛'), ('맛', '밥'), ('밥', '뉴턴'), ('뉴턴', '선생님'), ('선생님',))\n",
            "0.6\n",
            "0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pE8QFJMMk9aX"
      },
      "source": [
        "# 코사인 유사도"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUVrUlu1k-pS"
      },
      "source": [
        "import numpy as np\n",
        "from numpy import dot\n",
        "from numpy.linalg import norm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A56ev8YGl_Vq"
      },
      "source": [
        "# 코사인 유사도 계산\n",
        "def cos_sim(vec1, vec2):\n",
        "    return dot(vec1, vec2) / (norm(vec1) * norm(vec2))\n",
        "\n",
        "\n",
        "# TDM 만들기\n",
        "def make_term_doc_mat(sentence_bow, word_dics):\n",
        "    freq_mat = {}\n",
        "\n",
        "    for word in word_dics:\n",
        "        freq_mat[word] = 0\n",
        "\n",
        "    for word in word_dics:\n",
        "        if word in sentence_bow:\n",
        "            freq_mat[word] += 1\n",
        "\n",
        "    return freq_mat\n",
        "\n",
        "\n",
        "# 단어 벡터 만들기\n",
        "def make_vector(tdm):\n",
        "    vec = []\n",
        "    for key in tdm:\n",
        "        vec.append(tdm[key])\n",
        "    return vec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8zFvghiTmBYG",
        "outputId": "39e13978-8db5-499d-d6f7-7c6e05f3b7e9"
      },
      "source": [
        "# 문장 정의\n",
        "sentence1 = '6월에 뉴턴은 선생님의 제안으로 트리니티에 입학하였다'\n",
        "sentence2 = '6월에 뉴턴은 선생님의 제안으로 대학교에 입학하였다'\n",
        "sentence3 = '나는 맛잇는 밥을 뉴턴 선생님과 함께 먹었습니다.'\n",
        "\n",
        "# 헝태소분석기를 이용해 단어 묶음 리스트 생성\n",
        "okt = Okt()\n",
        "bow1 = okt.nouns(sentence1)\n",
        "bow2 = okt.nouns(sentence2)\n",
        "bow3 = okt.nouns(sentence3)\n",
        "\n",
        "# 단어 묶음 리스트를 하나로 합침\n",
        "bow = bow1 + bow2 + bow3\n",
        "\n",
        "# 단어 묶음에서 중복제거해 단어 사전 구축\n",
        "word_dics = []\n",
        "for token in bow:\n",
        "    if token not in word_dics:\n",
        "        word_dics.append(token)\n",
        "\n",
        "\n",
        "# 문장 별 단어 문서 행렬 계산\n",
        "freq_list1 = make_term_doc_mat(bow1, word_dics)\n",
        "freq_list2 = make_term_doc_mat(bow2, word_dics)\n",
        "freq_list3 = make_term_doc_mat(bow3, word_dics)\n",
        "print(freq_list1)\n",
        "print(freq_list2)\n",
        "print(freq_list3)\n",
        "\n",
        "\n",
        "# 코사인 유사도 계산\n",
        "doc1 = np.array(make_vector(freq_list1))\n",
        "doc2 = np.array(make_vector(freq_list2))\n",
        "doc3 = np.array(make_vector(freq_list3))\n",
        "r1 = cos_sim(doc1, doc2)\n",
        "r2 = cos_sim(doc3, doc1)\n",
        "print(r1)\n",
        "print(r2)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'뉴턴': 1, '선생님': 1, '제안': 1, '트리니티': 1, '입학': 1, '대학교': 0, '나': 0, '맛': 0, '밥': 0}\n",
            "{'뉴턴': 1, '선생님': 1, '제안': 1, '트리니티': 0, '입학': 1, '대학교': 1, '나': 0, '맛': 0, '밥': 0}\n",
            "{'뉴턴': 1, '선생님': 1, '제안': 0, '트리니티': 0, '입학': 0, '대학교': 0, '나': 1, '맛': 1, '밥': 1}\n",
            "0.7999999999999998\n",
            "0.3999999999999999\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Qygk2OzpGAr"
      },
      "source": [
        "https://soyoung-new-challenge.tistory.com/34"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TcHhwV9_ozXd"
      },
      "source": [
        "sentence = ( \"휴일 인 오늘 도 서쪽 을 중심 으로 폭염 이 이어졌는데요, 내일 은 반가운 비 소식 이 있습니다.\",\n",
        "\t     \"폭염 을 피해서 휴일 에 놀러왔다가 갑작스런 비 로 인해 망연자실 하고 있습니다.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_sg-ealRo5jh"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# 문장 벡터화\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(sentence)\n",
        "\n",
        "# 각 단어\n",
        "text = tfidf_vectorizer.get_feature_names()\n",
        "\n",
        "# 각 단어의 벡터 값\n",
        "idf = tfidf_vectorizer.idf_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uaLOLZ94o-qm",
        "outputId": "1c6ad9ac-b94a-4e16-bf76-f5a7874418ff"
      },
      "source": [
        "print(dict(zip(text, idf)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'갑작스런': 1.4054651081081644, '내일': 1.4054651081081644, '놀러왔다가': 1.4054651081081644, '망연자실': 1.4054651081081644, '반가운': 1.4054651081081644, '서쪽': 1.4054651081081644, '소식': 1.4054651081081644, '오늘': 1.4054651081081644, '으로': 1.4054651081081644, '이어졌는데요': 1.4054651081081644, '인해': 1.4054651081081644, '있습니다': 1.0, '중심': 1.4054651081081644, '폭염': 1.0, '피해서': 1.4054651081081644, '하고': 1.4054651081081644, '휴일': 1.0}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Mgy8tmApKv5"
      },
      "source": [
        "## 자카드 유사도( Jaccard Similarity)\n",
        "- 두 문장을 각각 단어의 집합으로 만든 뒤 두 집합을 통해 유사도를 측정하는 방식 중 하나\n",
        "- 유사도를 측정하는 방법은 두 집합의 교집합인 공통된 단어의 개수를 두 집합의 합집합, 전체 단어의 갯수로 나눈다.\n",
        "- 결과값은 공통의 원소의 개수에 따라 0과 1사이의 값이 나올 것이고, 1에 가까울수록 유사도가 높다"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ce-r13OrpYTS"
      },
      "source": [
        "## 코사인 유사도"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gPs7BK46pcju",
        "outputId": "5857b3cb-10c4-4642-e0ee-38b4ba4f9d65"
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.17952266]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOjOTXFhpk3s"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTbH4Rtaph6r"
      },
      "source": [
        "## 유클리디언 유사도 ( L2 Distance )"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4vFNmSeplgJ",
        "outputId": "9ce72b2e-791a-416e-9a54-480e45fbef0a"
      },
      "source": [
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "\n",
        "euclidean_distances(tfidf_matrix[0:1], tfidf_matrix[1:2])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.28099753]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z00wCXq8p1Wt"
      },
      "source": [
        "### 벡터를 일반화 + 유클리디언 유사도 \n",
        "- 앞서 확인했던 유사도 방식들은 모두 0과 1사이의 값을 가졌는데, 유클리디언 유사도는 1보다 큰 값이 나왔다.\n",
        "\n",
        "- 유클리디언 유사도는 단순히 두 점 사이의 거리를 뜻하므로 값에 제한이 없다.\n",
        "\n",
        "- 다른 유사도와 비교하려면 0과 1사이의 값으로 맞춰주어야한다.\n",
        "\n",
        "- 앞서 구한 벡터를 일반화한 후 다시 유클리디언 유사도를 측정하면 0과 1사이의 값을 가진다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bN2NPW1ZmRjF",
        "outputId": "5cb476ff-4bcb-4cab-b3ad-0b924c44d9d9"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def l1_normalize(v):\n",
        "    norm = np.sum(v)\n",
        "    return v / norm\n",
        "    \n",
        "    \n",
        "tfidf_norm_l1 = l1_normalize(tfidf_matrix)\n",
        "\n",
        "euclidean_distances(tfidf_norm_l1[0:1], tfidf_norm_l1[1:2])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.20491229]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kVbT6OdqIxR"
      },
      "source": [
        "## 맨하탄 유사도 ( L1 Distance )"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rpF6Ks2bpxlU",
        "outputId": "2b139898-9056-4042-9b5d-455cf95a3bb9"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import manhattan_distances\n",
        "\n",
        "def l1_normalize(v):\n",
        "    norm = np.sum(v)\n",
        "    return v / norm \n",
        "  \n",
        "# L1 정규화  \n",
        "tfidf_norm_l1 = l1_normalize(tfidf_matrix)\n",
        "\n",
        "manhattan_distances(tfidf_norm_l1[0:1], tfidf_norm_l1[1:2])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.77865927]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGa75X3KqVfK"
      },
      "source": [
        "## 맨하탄 유사도로 측정했을 때 유사도가 가장 높게 나왔다. 측정 방법에 따라 크게 유사도가 달라질 수 있으므로, 의도하고자 하는 방향에 맞는 유사도측정 방법을 고르는 것이 중요하다"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdPl95-9qMwe"
      },
      "source": [
        "## - 맨하탄 유사도로 측정했을 때 유사도가 가장 높게 나왔다. 측정 방법에 따라 크게 유사도가 달라질 수 있으므로, 의도하고자 하는 방향에 맞는 유사도측정 방법을 고르는 것이 중요하다"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "neQTm2J0qUbo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}