{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hi-WoF8NsAP2"
   },
   "source": [
    "# 한국어 전처리\n",
    "\n",
    "\n",
    "> Smilegate.AI Human-like AI part 김성현 (banaband657@gmail.com)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yaMc1sJMVuga"
   },
   "source": [
    "## 0. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t3zB8rv8NCDf"
   },
   "source": [
    "한국어에서의 다양한 전처리 방식들을 실습합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l2stY5xZik0S"
   },
   "source": [
    "* Basic\n",
    " - 가장 기초적인 전처리\n",
    " - html tag 제거\n",
    " - 숫자 제거\n",
    " - Lowercasing\n",
    " - \"@%*=()/+ 와 같은 punctuation 제거\n",
    "* Spell check\n",
    " - 사전 기반의 오탈자 교정\n",
    " - 줄임말 원형 복원 (e.g. I'm not happy -> I am not happy)\n",
    "* Part-of-Speech\n",
    " - 형태소 분석\n",
    " - Noun, Adjective, Verb, Adverb만 학습에 사용\n",
    "* Stemming\n",
    " - 형태소 분석 이후 동사 원형 복원\n",
    "* Stopwords\n",
    " - 불용어 제거\n",
    "* Negation\n",
    " - [논문](https://dl.acm.org/doi/pdf/10.5555/2392701.2392703)\n",
    " - 부정 표현에 대한 단순화 (e.g. I'm not happy -> I'm sad)\n",
    " - 한국어에서의 적용이 어려워, 추후 추가할 예정\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h9SGo8FYJ2yp"
   },
   "source": [
    "먼저 실습을 위해 한국어 wikipedia 문서를 다운받도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "id": "4hdEumzOJ_Kn",
    "outputId": "0dc5d01a-a3d9-44db-e5bb-86a6e4b14b5c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100  1555  100  1555    0     0   1555      0  0:00:01 --:--:--  0:00:01  7133\n"
     ]
    }
   ],
   "source": [
    "!curl -c ./cookie -s -L \"https://drive.google.com/uc?export=download&id=1EcJpRTEdGVaYhbLE1otE5iCifj_kW1_4\" > /dev/null\n",
    "!curl -Lb ./cookie \"https://drive.google.com/uc?export=download&confirm=`awk '/download/ {print $NF}' ./cookie`&id=1EcJpRTEdGVaYhbLE1otE5iCifj_kW1_4\" -o wiki_20190620.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1zAzPWzWVtN4"
   },
   "source": [
    "## 1. Basic Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "bPxWFhdVhY-b"
   },
   "outputs": [],
   "source": [
    "# 한국어 위키 데이터 load\n",
    "# data = open('/content/wiki_20190620.txt', 'r', encoding='utf-8')\n",
    "data = open('./wiki_20190620.txt', 'r', encoding='utf-8')\n",
    "lines = data.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 389
    },
    "id": "8nIXezslMdDC",
    "outputId": "316b92c0-0c36-4f69-eef3-885ad8a750a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "\n",
      "<html lang=en>\n",
      "\n",
      "  <meta charset=utf-8>\n",
      "\n",
      "  <meta name=viewport content=\"initial-scale=1, minimum-scale=1, width=device-width\">\n",
      "\n",
      "  <title>Error 400 (Bad Request)!!1</title>\n",
      "\n",
      "  <style>\n",
      "\n",
      "    *{margin:0;padding:0}html,code{font:15px/22px arial,sans-serif}html{background:#fff;color:#222;padding:15px}body{margin:7% auto 0;max-width:390px;min-height:180px;padding:30px 0 15px}* > body{background:url(//www.google.com/images/errors/robot.png) 100% 5px no-repeat;padding-right:205px}p{margin:11px 0 22px;overflow:hidden}ins{color:#777;text-decoration:none}a img{border:0}@media screen and (max-width:772px){body{background:none;margin-top:0;max-width:none;padding-right:0}}#logo{background:url(//www.google.com/images/branding/googlelogo/1x/googlelogo_color_150x54dp.png) no-repeat;margin-left:-5px}@media only screen and (min-resolution:192dpi){#logo{background:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) no-repeat 0% 0%/100% 100%;-moz-border-image:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) 0}}@media only screen and (-webkit-min-device-pixel-ratio:2){#logo{background:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) no-repeat;-webkit-background-size:100% 100%}}#logo{display:inline-block;height:54px;width:150px}\n",
      "\n",
      "  </style>\n",
      "\n",
      "  <a href=//www.google.com/><span id=logo aria-label=Google></span></a>\n",
      "\n",
      "  <p><b>400.</b> <ins>That’s an error.</ins>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 10):\n",
    "    print(lines[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AVHfKvlTKRom"
   },
   "source": [
    "한국어 문장 분리 라이브러리 중, 가장 성능이 좋은 tokenizer 중 하나인 kss를 설치합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "iFecdharYe41",
    "outputId": "a7009822-a0e2-4be3-cd86-2ce9ff5a7f8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kss in /usr/local/lib/python3.6/dist-packages (1.3.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install kss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pe6q853nYZ32"
   },
   "outputs": [],
   "source": [
    "import kss\n",
    "\n",
    "sentence_tokenized_text = []\n",
    "for i, line in enumerate(lines):\n",
    "    if i > 100:     # 전체 wikipedia 문서는 사이즈가 크므로, 일부만 테스트.\n",
    "        break\n",
    "    line = line.strip()\n",
    "    for sent in kss.split_sentences(line):\n",
    "        sentence_tokenized_text.append(sent.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HCh0BLJdLJDZ"
   },
   "source": [
    "이제 `sentence_tokenized_text`에 문장 단위로 분리된 corpus가 저장되었습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8wTd_6S9W_3i"
   },
   "outputs": [],
   "source": [
    "punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p25OOozUYJV6"
   },
   "outputs": [],
   "source": [
    "punct_mapping = {\"‘\": \"'\", \"₹\": \"e\", \"´\": \"'\", \"°\": \"\", \"€\": \"e\", \"™\": \"tm\", \"√\": \" sqrt \", \"×\": \"x\", \"²\": \"2\", \"—\": \"-\", \"–\": \"-\", \"’\": \"'\", \"_\": \"-\", \"`\": \"'\", '“': '\"', '”': '\"', '“': '\"', \"£\": \"e\", '∞': 'infinity', 'θ': 'theta', '÷': '/', 'α': 'alpha', '•': '.', 'à': 'a', '−': '-', 'β': 'beta', '∅': '', '³': '3', 'π': 'pi', }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RHJ2_j9kYLco"
   },
   "outputs": [],
   "source": [
    "def clean_punc(text, punct, mapping):\n",
    "    for p in mapping:\n",
    "        text = text.replace(p, mapping[p])\n",
    "    \n",
    "    for p in punct:\n",
    "        text = text.replace(p, f' {p} ')\n",
    "    \n",
    "    specials = {'\\u200b': ' ', '…': ' ... ', '\\ufeff': '', 'करना': '', 'है': ''}\n",
    "    for s in specials:\n",
    "        text = text.replace(s, specials[s])\n",
    "    \n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FFLuqnyhNBPS"
   },
   "outputs": [],
   "source": [
    "cleaned_corpus = []\n",
    "for sent in sentence_tokenized_text:\n",
    "    cleaned_corpus.append(clean_punc(sent, punct, punct_mapping))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 213
    },
    "id": "4Im-PMTRNQJz",
    "outputId": "0d32728a-7315-45a1-b64e-6597cbc3fc8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "제임스 얼   \"  지미  \"   카터 주니어는 민주당 출신 미국 39번째 대통령 이다 .\n",
      "지미 카터는 조지아주 섬터 카운티 플레인스 마을에서 태어났다 .\n",
      "조지아 공과대학교를 졸업하였다 .\n",
      "그 후 해군에 들어가 전함·원자력·잠수함의 승무원으로 일하였다 .\n",
      "1953년 미국 해군 대위로 예편하였고 이후 땅콩·면화 등을 가꿔 많은 돈을 벌었다 .\n",
      "그의 별명이   \"  땅콩 농부  \"   로 알려졌다 .\n",
      "1962년 조지아 주 상원 의원 선거에서 낙선하나 그 선거가 부정선거 였음을 입증하게 되어 당선되고 ,  1966년 조지아 주 지사 선거에 낙선하지만 1970년 조지아 주 지사를 역임했다 .\n",
      "대통령이 되기 전 조지아주 상원의원을 두번 연임했으며 ,  1971년부터 1975년까지 조지아 지사로 근무했다 .\n",
      "조지아 주지사로 지내면서 ,  미국에 사는 흑인 등용법을 내세웠다 .\n",
      "1976년 대통령 선거에 민주당 후보로 출마하여 도덕주의 정책으로 내세워 ,  포드를 누르고 당선되었다 .\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 10):\n",
    "    print(cleaned_corpus[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fVSpOZIoYOOJ"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(texts):\n",
    "    corpus = []\n",
    "    for i in range(0, len(texts)):\n",
    "        review = re.sub(r'[@%\\\\*=()/~#&\\+á?\\xc3\\xa1\\-\\|\\.\\:\\;\\!\\-\\,\\_\\~\\$\\'\\\"]', '',str(texts[i])) #remove punctuation\n",
    "        review = re.sub(r'\\d+','', str(texts[i]))# remove number\n",
    "        review = review.lower() #lower case\n",
    "        review = re.sub(r'\\s+', ' ', review) #remove extra space\n",
    "        review = re.sub(r'<[^>]+>','',review) #remove Html tags\n",
    "        review = re.sub(r'\\s+', ' ', review) #remove spaces\n",
    "        review = re.sub(r\"^\\s+\", '', review) #remove space from start\n",
    "        review = re.sub(r'\\s+$', '', review) #remove space from the end\n",
    "        corpus.append(review)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wwvWW5AuOIFS"
   },
   "outputs": [],
   "source": [
    "basic_preprocessed_corpus = clean_text(cleaned_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 213
    },
    "id": "8JXT1xXdOaMh",
    "outputId": "f70bf3f6-2c89-44cc-8a0c-adfe4f5c73f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "제임스 얼 \" 지미 \" 카터 주니어는 민주당 출신 미국 번째 대통령 이다 .\n",
      "지미 카터는 조지아주 섬터 카운티 플레인스 마을에서 태어났다 .\n",
      "조지아 공과대학교를 졸업하였다 .\n",
      "그 후 해군에 들어가 전함·원자력·잠수함의 승무원으로 일하였다 .\n",
      "년 미국 해군 대위로 예편하였고 이후 땅콩·면화 등을 가꿔 많은 돈을 벌었다 .\n",
      "그의 별명이 \" 땅콩 농부 \" 로 알려졌다 .\n",
      "년 조지아 주 상원 의원 선거에서 낙선하나 그 선거가 부정선거 였음을 입증하게 되어 당선되고 , 년 조지아 주 지사 선거에 낙선하지만 년 조지아 주 지사를 역임했다 .\n",
      "대통령이 되기 전 조지아주 상원의원을 두번 연임했으며 , 년부터 년까지 조지아 지사로 근무했다 .\n",
      "조지아 주지사로 지내면서 , 미국에 사는 흑인 등용법을 내세웠다 .\n",
      "년 대통령 선거에 민주당 후보로 출마하여 도덕주의 정책으로 내세워 , 포드를 누르고 당선되었다 .\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 10):\n",
    "    print(basic_preprocessed_corpus[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "REdDar2RQyN5"
   },
   "source": [
    "## 2. Spell check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VrkpMW2sW8if"
   },
   "source": [
    "띄어쓰기 검사로는 [한국어 띄어쓰기 검사 라이브러리](https://github.com/haven-jeon/PyKoSpacing)를 사용하고,   \n",
    "맞춤법 검사로는 [한국어 맞춤법 검사 라이브러리](https://github.com/ssut/py-hanspell)와, [논문](https://link.springer.com/chapter/10.1007/978-3-030-12385-7_3)에서 사용되었던 외래어 사전을 사용하겠습니다.   \n",
    "반복되는 이모티콘이나 자소는 이 [라이브러리](https://github.com/lovit/soynlp)를 이용해 필터링 하겠습니다.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v8vzn-5TV8f6"
   },
   "source": [
    "먼저 띄어쓰기 검사기를 설치하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 899
    },
    "id": "lxlDMy8oV-oP",
    "outputId": "8089c99a-d95c-4ab8-f14d-dd807e6fdbda"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/haven-jeon/PyKoSpacing.git\n",
      "  Cloning https://github.com/haven-jeon/PyKoSpacing.git to /tmp/pip-req-build-oqqkbrnq\n",
      "  Running command git clone -q https://github.com/haven-jeon/PyKoSpacing.git /tmp/pip-req-build-oqqkbrnq\n",
      "Requirement already satisfied (use --upgrade to upgrade): pykospacing==0.2 from git+https://github.com/haven-jeon/PyKoSpacing.git in /usr/local/lib/python3.6/dist-packages\n",
      "Requirement already satisfied: tensorflow>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from pykospacing==0.2) (2.3.0)\n",
      "Requirement already satisfied: keras>=2.4.3 in /usr/local/lib/python3.6/dist-packages (from pykospacing==0.2) (2.4.3)\n",
      "Requirement already satisfied: h5py>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from pykospacing==0.2) (2.10.0)\n",
      "Requirement already satisfied: argparse>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from pykospacing==0.2) (1.4.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->pykospacing==0.2) (1.1.0)\n",
      "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->pykospacing==0.2) (0.3.3)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->pykospacing==0.2) (1.12.1)\n",
      "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->pykospacing==0.2) (1.1.2)\n",
      "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->pykospacing==0.2) (0.2.0)\n",
      "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->pykospacing==0.2) (1.18.5)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->pykospacing==0.2) (3.3.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->pykospacing==0.2) (1.15.0)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->pykospacing==0.2) (0.9.0)\n",
      "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->pykospacing==0.2) (1.4.1)\n",
      "Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->pykospacing==0.2) (2.3.0)\n",
      "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->pykospacing==0.2) (1.6.3)\n",
      "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->pykospacing==0.2) (2.3.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->pykospacing==0.2) (0.34.2)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->pykospacing==0.2) (1.30.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->pykospacing==0.2) (3.12.2)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras>=2.4.3->pykospacing==0.2) (3.13)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=2.3.0->pykospacing==0.2) (2.23.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=2.3.0->pykospacing==0.2) (1.7.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=2.3.0->pykospacing==0.2) (49.1.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=2.3.0->pykospacing==0.2) (1.0.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=2.3.0->pykospacing==0.2) (1.17.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=2.3.0->pykospacing==0.2) (3.2.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=2.3.0->pykospacing==0.2) (0.4.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow>=2.3.0->pykospacing==0.2) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow>=2.3.0->pykospacing==0.2) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow>=2.3.0->pykospacing==0.2) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow>=2.3.0->pykospacing==0.2) (3.0.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow>=2.3.0->pykospacing==0.2) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow>=2.3.0->pykospacing==0.2) (4.6)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow>=2.3.0->pykospacing==0.2) (4.1.1)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow>=2.3.0->pykospacing==0.2) (1.7.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow>=2.3.0->pykospacing==0.2) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow>=2.3.0->pykospacing==0.2) (0.4.8)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow>=2.3.0->pykospacing==0.2) (3.1.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow>=2.3.0->pykospacing==0.2) (3.1.0)\n",
      "Building wheels for collected packages: pykospacing\n",
      "  Building wheel for pykospacing (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pykospacing: filename=pykospacing-0.2-cp36-none-any.whl size=2255584 sha256=ee7f9561c80ba93bce5f5b0734c2f6649610fc910c392985b1c03e61b08fdbd7\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-j_a9aj1g/wheels/4d/45/58/e26cb2b7f6a063d234158c6fd1e5700f6e15b99d67154340ba\n",
      "Successfully built pykospacing\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/haven-jeon/PyKoSpacing.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "8c1xehl2QVVd",
    "outputId": "c713a5a7-27d5-4da1-a1bb-64c71db37b7c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"김형호 영화시장 분석가는 '1987'의 네이버 영화 정보 네티즌 10점 평에서 언급된 단어들을 지난해 12월 27일부터 올해 1월 10일까지 통계 프로그램 R과 KoNLP 패키지로 텍스트마이닝하여 분석했다.\""
      ]
     },
     "execution_count": 152,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pykospacing import spacing\n",
    "spacing(\"김형호영화시장분석가는'1987'의네이버영화정보네티즌10점평에서언급된단어들을지난해12월27일부터올해1월10일까지통계프로그램R과KoNLP패키지로텍스트마이닝하여분석했다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8i2UY1EHSIcd"
   },
   "source": [
    "다음으로 맞춤법 검사기를 설치하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "8c_1H4EyWLMG",
    "outputId": "882e0907-3bf2-4420-e99c-ffce56d4a2f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/ssut/py-hanspell.git\n",
      "  Cloning https://github.com/ssut/py-hanspell.git to /tmp/pip-req-build-n5eem4mz\n",
      "  Running command git clone -q https://github.com/ssut/py-hanspell.git /tmp/pip-req-build-n5eem4mz\n",
      "Requirement already satisfied (use --upgrade to upgrade): py-hanspell==1.1 from git+https://github.com/ssut/py-hanspell.git in /usr/local/lib/python3.6/dist-packages\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from py-hanspell==1.1) (2.23.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->py-hanspell==1.1) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->py-hanspell==1.1) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->py-hanspell==1.1) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->py-hanspell==1.1) (1.24.3)\n",
      "Building wheels for collected packages: py-hanspell\n",
      "  Building wheel for py-hanspell (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for py-hanspell: filename=py_hanspell-1.1-cp36-none-any.whl size=4854 sha256=21de0d128296407500a81bccb28be91da451efb7b03b3e493423fa642bbc8642\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-vqkcam1h/wheels/0a/25/d1/e5e96476dbb1c318cc26c992dd493394fe42b0c204b3e65588\n",
      "Successfully built py-hanspell\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/ssut/py-hanspell.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "kOhGie5wUR3M",
    "outputId": "3acc7df7-52b0-4589-809c-457071128df2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "대체 왜 안되는지 설명을 해봐\n"
     ]
    }
   ],
   "source": [
    "from hanspell import spell_checker\n",
    " \n",
    "sent = \"대체 왜 않돼는지 설명을 해바\"\n",
    "spelled_sent = spell_checker.check(sent)\n",
    "checked_sent = spelled_sent.checked\n",
    " \n",
    "print(checked_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DTRPwylDYY3_"
   },
   "source": [
    "다음으로는 데이터에서 반복되는 이모티콘이나 자모를 normalization을 위한 라이브러리를 설치하도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "id": "wGfCqUaRYW2C",
    "outputId": "bb869ad0-e6b7-4560-ba8b-9018e101d783"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: soynlp in /usr/local/lib/python3.6/dist-packages (0.0.493)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from soynlp) (1.4.1)\n",
      "Requirement already satisfied: psutil>=5.0.1 in /usr/local/lib/python3.6/dist-packages (from soynlp) (5.4.8)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.6/dist-packages (from soynlp) (0.22.2.post1)\n",
      "Requirement already satisfied: numpy>=1.12.1 in /usr/local/lib/python3.6/dist-packages (from soynlp) (1.18.5)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.20.0->soynlp) (0.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install soynlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "qthNSNCeYolf",
    "outputId": "fa0b38a0-b9fb-4906-9331-02baf0a5088e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "와하하핫\n"
     ]
    }
   ],
   "source": [
    "from soynlp.normalizer import *\n",
    "print(repeat_normalize('와하하하하하하하하하핫', num_repeats=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7xWaCy7ZX_R6"
   },
   "source": [
    "마지막으로 외래어 사전을 다운로드 받겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "id": "1aGKJHKJYEk7",
    "outputId": "52aec4e0-9db5-4f67-8a50-6ee301182050"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\r",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r",
      "100   408    0   408    0     0   1022      0 --:--:-- --:--:-- --:--:--  1020\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100 19779  100 19779    0     0  17883      0  0:00:01  0:00:01 --:--:--     0\n"
     ]
    }
   ],
   "source": [
    "!curl -c ./cookie -s -L \"https://drive.google.com/uc?export=download&id=1RNYpLE-xbMCGtiEHIoNsCmfcyJP3kLYn\" > /dev/null\n",
    "!curl -Lb ./cookie \"https://drive.google.com/uc?export=download&confirm=`awk '/download/ {print $NF}' ./cookie`&id=1RNYpLE-xbMCGtiEHIoNsCmfcyJP3kLYn\" -o confused_loanwords.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FunWsqO-Yb5H"
   },
   "outputs": [],
   "source": [
    "lownword_map = {}\n",
    "lownword_data = open('/content/confused_loanwords.txt', 'r', encoding='utf-8')\n",
    "\n",
    "lines = lownword_data.readlines()\n",
    "\n",
    "for line in lines:\n",
    "    line = line.strip()\n",
    "    miss_spell = line.split('\\t')[0]\n",
    "    ori_word = line.split('\\t')[1]\n",
    "    lownword_map[miss_spell] = ori_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xOcOPy82Z5Qa"
   },
   "outputs": [],
   "source": [
    "def spell_check_text(texts):\n",
    "    corpus = []\n",
    "    for sent in texts:\n",
    "        spaced_text = spacing(sent)\n",
    "        spelled_sent = spell_checker.check(sent)\n",
    "        checked_sent = spelled_sent.checked\n",
    "        normalized_sent = repeat_normalize(checked_sent)\n",
    "        for lownword in lownword_map:\n",
    "            normalized_sent = normalized_sent.replace(lownword, lownword_map[lownword])\n",
    "        corpus.append(normalized_sent)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wDxvxz0YbOn9"
   },
   "outputs": [],
   "source": [
    "spell_preprocessed_corpus = spell_check_text(basic_preprocessed_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "agigst8ojDKo"
   },
   "source": [
    "## 3. Part-of-Speech "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ic53Jk-JjHh6"
   },
   "source": [
    "Python 기반의 형태소 분석기 중, 성능이 가장 좋은 것 중 하나인 카카오의 [Khaiii](https://github.com/kakao/khaiii)를 사용하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Rwma8M5gjW5L",
    "outputId": "4b8acc8f-26db-45ca-c188-501197613c37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'khaiii' already exists and is not an empty directory.\n",
      "Requirement already satisfied: cmake in /usr/local/lib/python3.6/dist-packages (3.12.0)\n",
      "mkdir: cannot create directory ‘build’: File exists\n",
      "-- [khaiii] fused multiply add option enabled\n",
      "-- [hunter] Calculating Toolchain-SHA1\n",
      "-- [hunter] Calculating Config-SHA1\n",
      "-- [hunter] HUNTER_ROOT: /root/.hunter\n",
      "-- [hunter] [ Hunter-ID: 70287b1 | Toolchain-ID: 02ccb06 | Config-ID: dffbc08 ]\n",
      "-- [hunter] BOOST_ROOT: /root/.hunter/_Base/70287b1/02ccb06/dffbc08/Install (ver.: 1.68.0-p1)\n",
      "-- Boost version: 1.68.0\n",
      "-- [hunter] CXXOPTS_ROOT: /root/.hunter/_Base/70287b1/02ccb06/dffbc08/Install (ver.: 2.1.1-pre)\n",
      "-- [hunter] EIGEN_ROOT: /root/.hunter/_Base/70287b1/02ccb06/dffbc08/Install (ver.: 3.3.5)\n",
      "-- [hunter] FMT_ROOT: /root/.hunter/_Base/70287b1/02ccb06/dffbc08/Install (ver.: 4.1.0)\n",
      "-- [hunter] GTEST_ROOT: /root/.hunter/_Base/70287b1/02ccb06/dffbc08/Install (ver.: 1.8.0-hunter-p11)\n",
      "-- [hunter] NLOHMANN_JSON_ROOT: /root/.hunter/_Base/70287b1/02ccb06/dffbc08/Install (ver.: 3.3.0)\n",
      "-- [hunter] SPDLOG_ROOT: /root/.hunter/_Base/70287b1/02ccb06/dffbc08/Install (ver.: 0.16.3-p1)\n",
      "-- Configuring done\n",
      "-- Generating done\n",
      "-- Build files have been written to: /content/build\n",
      "[ 65%] Built target obj_khaiii\n",
      "[ 69%] Built target khaiii\n",
      "[ 76%] Built target bin_khaiii\n",
      "[100%] Built target test_khaiii\n",
      "Built target resource\n",
      "[ 65%] Built target obj_khaiii\n",
      "[ 69%] Built target khaiii\n",
      "[ 76%] Built target bin_khaiii\n",
      "[100%] Built target test_khaiii\n",
      "\u001b[36mInstall the project...\u001b[0m\n",
      "-- Install configuration: \"\"\n",
      "-- Up-to-date: /usr/local/include/khaiii\n",
      "-- Up-to-date: /usr/local/include/khaiii/KhaiiiApi.hpp\n",
      "-- Up-to-date: /usr/local/include/khaiii/khaiii_dev.h\n",
      "-- Up-to-date: /usr/local/include/khaiii/khaiii_api.h\n",
      "-- Up-to-date: /usr/local/share/khaiii\n",
      "-- Up-to-date: /usr/local/share/khaiii/restore.val\n",
      "-- Up-to-date: /usr/local/share/khaiii/errpatch.tri\n",
      "-- Up-to-date: /usr/local/share/khaiii/preanal.tri\n",
      "-- Up-to-date: /usr/local/share/khaiii/errpatch.val\n",
      "-- Up-to-date: /usr/local/share/khaiii/conv.3.fil\n",
      "-- Up-to-date: /usr/local/share/khaiii/conv.5.fil\n",
      "-- Up-to-date: /usr/local/share/khaiii/preanal.val\n",
      "-- Up-to-date: /usr/local/share/khaiii/embed.bin\n",
      "-- Up-to-date: /usr/local/share/khaiii/config.json\n",
      "-- Up-to-date: /usr/local/share/khaiii/conv.4.fil\n",
      "-- Up-to-date: /usr/local/share/khaiii/hdn2tag.lin\n",
      "-- Up-to-date: /usr/local/share/khaiii/restore.key\n",
      "-- Up-to-date: /usr/local/share/khaiii/restore.one\n",
      "-- Up-to-date: /usr/local/share/khaiii/cnv2hdn.lin\n",
      "-- Up-to-date: /usr/local/share/khaiii/errpatch.len\n",
      "-- Up-to-date: /usr/local/share/khaiii/conv.2.fil\n",
      "-- Up-to-date: /usr/local/lib/libkhaiii.so.0.4\n",
      "-- Up-to-date: /usr/local/lib/libkhaiii.so.0\n",
      "-- Up-to-date: /usr/local/lib/libkhaiii.so\n",
      "-- Up-to-date: /usr/local/bin/khaiii\n",
      "\u001b[36mRun CPack packaging tool for source...\u001b[0m\n",
      "CPack: Create package using ZIP\n",
      "CPack: Install projects\n",
      "CPack: - Install directory: /content/khaiii\n",
      "CPack: Create package\n",
      "CPack: - package: /content/build/khaiii-0.4.zip generated.\n",
      "Built target package_python\n",
      "Processing ./build/package_python\n",
      "Building wheels for collected packages: khaiii\n",
      "  Building wheel for khaiii (setup.py) ... \u001b[?25l\u001b[?25hcanceled\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/kakao/khaiii.git\n",
    "!pip install cmake\n",
    "!mkdir build\n",
    "!cd build && cmake /content/khaiii\n",
    "!cd /content/build/ && make all\n",
    "!cd /content/build/ && make resource\n",
    "!cd /content/build && make install\n",
    "!cd /content/build && make package_python\n",
    "!pip install /content/build/package_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "wAGMx7ZnmTWW",
    "outputId": "062df4ad-acd5-4d41-d919-6db252d492db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "나/NP\n",
      "도/JX\n",
      "모르/VV\n",
      "게/EC\n",
      "사/VV\n",
      "아/EC\n",
      "버리/VX\n",
      "었/EP\n",
      "다/EF\n",
      "./SF\n",
      "\n",
      "\n",
      "행복/NNG\n",
      "하/XSA\n",
      "아/EF\n",
      "야/EC\n",
      "하/VX\n",
      "아/EF\n",
      "!/SF\n",
      "\n",
      "\n",
      "내/NP\n",
      "가/JKS\n",
      "안/MAG\n",
      "그/VV\n",
      "렇/VA\n",
      "었/EP\n",
      "어/EF\n",
      "!/SF\n",
      "\n",
      "\n",
      "나/NP\n",
      "는/JX\n",
      "사/VV\n",
      "지/EC\n",
      "않/VX\n",
      "았/EP\n",
      "어/EF\n",
      "./SF\n",
      "\n",
      "\n",
      "하나/NR\n",
      "도/JX\n",
      "안/MAG\n",
      "기쁘/VA\n",
      "다/EF\n",
      "./SF\n",
      "\n",
      "\n",
      "상관/NNG\n",
      "하/XSV\n",
      "지마/NNG\n",
      "\n",
      "\n",
      "그것/NP\n",
      "좀/MAG\n",
      "가져오/VV\n",
      "아/EC\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from khaiii import KhaiiiApi\n",
    "api = KhaiiiApi()\n",
    "\n",
    "test_sents = [\"나도 모르게 사버렸다.\", \"행복해야해!\", \"내가 안 그랬어!\", \"나는 사지 않았어.\", \"하나도 안 기쁘다.\", \"상관하지마\", \"그것 좀 가져와\"]\n",
    "\n",
    "for sent in test_sents:\n",
    "    for word in api.analyze(sent):\n",
    "        for morph in word.morphs:\n",
    "            print(morph.lex + '/' + morph.tag)\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rVc1TUiBmM_b"
   },
   "outputs": [],
   "source": [
    "significant_tags = ['NNG', 'NNP', 'NNB', 'VV', 'VA', 'VX', 'MAG', 'MAJ', 'XSV', 'XSA']\n",
    "\n",
    "def pos_text(texts):\n",
    "    corpus = []\n",
    "    for sent in texts:\n",
    "        pos_tagged = ''\n",
    "        for word in api.analyze(sent):\n",
    "            for morph in word.morphs:\n",
    "                if morph.tag in significant_tags:\n",
    "                    pos_tagged += morph.lex + '/' + morph.tag + ' '\n",
    "        corpus.append(pos_tagged.strip())\n",
    "    return corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "afu1QA1oq5WY"
   },
   "outputs": [],
   "source": [
    "pos_tagged_corpus = pos_text(spell_preprocessed_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 565
    },
    "id": "3fF6kFFpq_D6",
    "outputId": "27352d82-68fb-439a-81d7-08bc12421512"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "제임스/NNP 얼/NNG 지/NNP 미/NNG 카터/NNP 주니/NNG 어/NNP 민주당/NNP 출신/NNG 미국/NNP 번/NNB 대통령/NNG\n",
      "지미/NNP 카터/NNP 조지아주/NNP 섬터/NNG 카운/NNG 티/NNP 플레인스/NNG 마을/NNG 태어나/VV\n",
      "조지아/NNP 공과/NNG 대학교/NNG 졸업/NNG 하/XSV\n",
      "후/NNG 해군/NNG 들어가/VV 전함/NNG 원자력/NNG 잠수/NNG 하/XSA 승무원/NNG 일/NNG 하/XSV\n",
      "년/NNB 미국/NNP 해군/NNG 대위/NNG 예편/NNG 하/XSA 이후/NNG 땅콩/NNG 면화/NNG 등/NNB 가꾸/VV 많/VA 돈/NNG 벌/VV\n",
      "별명/NNG 땅콩/NNG 농부/NNG 알리/VV 지/VX\n",
      "년/NNB 조지아/NNP 주/NNP 상원/NNG 의원/NNG 선거/NNG 낙선/NNG 하/XSV 선거/NNG 부정/NNG 선거/NNG 입증/NNG 하/XSV 되/VV 당선/NNG 되/XSV 년/NNB 조지아/NNP 주/NNG 지사/NNG 선거/NNG 낙선/NNG 하/XSV 년/NNB 조지아/NNP 주/NNG 지사/NNG 역임/NNG 하/XSV\n",
      "대통령/NNG 되/VV 전/NNG 조지아주/NNP 상원/NNG 의원/NNG 번/NNB 연임/NNG 하/XSV 년/NNB 년/NNB 조지아/NNP 지사/NNG 근무/NNG 하/XSV\n",
      "조지아/NNP 주지사/NNG 지내/VV 미국/NNP 살/VV 흑인/NNG 등용/NNG 법/NNG 내세우/VV\n",
      "년/NNB 대통령/NNG 선거/NNG 민주당/NNP 후보/NNG 출마/NNG 하/XSV 도덕주의/NNG 정책/NNG 내세우/VV 포/NNP 드/NNG 누르/VV 당선/NNG 되/XSV\n",
      "카터/NNP 대통령/NNG 에너지/NNG 개발/NNG 촉구/NNG 하/XSV 공화당/NNP 반대/NNG 무산/NNG 되/XSV\n",
      "카터/NNP 이집트/NNP 이스라엘/NNP 조정/NNG 하/XSV 캠프/NNG 데이비드/NNP 안와르/NNP 사다/NNP 트/NNG 대통령/NNG 메나헴/NNP 베기/VV 수상/NNG 함께/MAG 중동/NNP 평화/NNG 위하/VV 캠프/NNG 데이비드/NNP 협정/NNG 체결/NNG 하/XSV\n",
      "그러나/MAJ 공화당/NNP 미국/NNP 유대인/NNG 단체/NNG 반발/NNG 일으키/VV\n",
      "년/NNB 백악관/NNP 양국/NNG 간/NNB 평화/NNG 조약/NNG 이끌/VV 지/VX\n",
      "또한/MAG 소련/NNP 제차/NNG 전략/NNG 무기/NNG 제한/NNG 협상/NNG 조인/NNG 하/XSV\n",
      "카터/NNP 연대/NNG 후반/NNG 당시/NNG 대한민국/NNP 등/NNB 인권/NNG 후진국/NNG 국민/NNG 인권/NNG 지키/VV 위하/VV 노력/NNG 하/XSV 취임/NNG 이후/NNG 계속/NNG 하/XSV 도덕/NNG 정치/NNG 내세우/VV\n",
      "그러나/MAJ 이/NNP 미국/NNP 대사관/NNG 인질/NNG 사건/NNG 인질/NNG 구출/NNG 실패/NNG 이유/NNG 년/NNB 대통령/NNG 선거/NNG 공화당/NNP 로널드/NNP 레이건/NNP 후보/NNG 지/VV 결국/NNG 재선/NNG 실패/NNG 하/XSV\n",
      "또한/MAG 임기/NNG 말기/NNG 터지/VV 소련/NNP 아프가니스탄/NNP 침공/NNG 사건/NNG 인하/VV 년/NNB 하계/NNG 올림픽/NNG 반공/NNG 국가/NNG 보이콧/NNG 내세우/VV\n",
      "지미/NNP 카터/NNP 대한민국/NNP 관계/NNG 중요/NNG 하/XSA 영향/NNG 미치/VV 대통령/NNG 중/NNB\n",
      "인권/NNG 문제/NNG 주한/NNG 미군/NNG 철수/NNG 문제/NNG 한때/NNG 한미/NNP 관계/NNG 불편/NNG 하/XSA 하/VX\n",
      "년/NNB 대한민국/NNP 대하/VV 북한/NNP 위협/NNG 대비/NNG 하/XSV 한미연/NNP 합사/NNG 창설/NNG 하/XSV 년/NNB 단계/NNG 걸치/VV 주한/NNG 미군/NNG 철수/NNG 하/XSV 하/VV\n",
      "그러나/MAJ 주한/NNG 미군/NNG 사령부/NNG 정보기관/NNG 의회/NNG 반대/NNG 부딪히/VV 주한/NNG 미군/NNG 완전/NNG 철수/NNG 대신/NNG 명/NNG 감축/NNG 하/XSV 데/NNB 그치/VV\n",
      "또한/MAG 박정희/NNP 정권/NNG 인권/NNG 문제/NNG 등/NNB 논란/NNG 불협화음/NNG 내/VV 년/NNB 월/NNB 하순/NNG 대한민국/NNP 방문/NNG 하/XSV 관계/NNG 다소/MAG 회복/NNG 되/XSV\n",
      "년/NNB 년/NNB 대한민국/NNP 정치/NNG 격변기/NNG 당시/NNG 대통령/NNG 대하/VV 하/XSA 태도/NNG 보이/VV 후/NNG 대한민국/NNP 내/NNB 고조/NNG 되/XSV 미/NNP 운동/NNG 원인/NNG 되/VV\n",
      "월/NNG 일/NNG 박정희/NNP 대통령/NNG 김재규/NNP 중앙/NNG 정보/NNG 부장/NNG 의하/VV 살해/NNG 되/XSV 것/NNB 대하/VV 사건/NNG 크/VA 충격/NNG 받/VV 사/NNP 이러스/NNG 밴스/NNG 국무/NNG 장관/NNG 조문사절/NNG 파견/NNG 하/XSV\n",
      "군사/NNG 반란과/NNG 쿠데타/NNG 대하/VV 초기/NNG 강하/VA 비난/NNG 하/XSV 미국/NNP 정부/NNG 신군부/NNG 설득/NNG 하/XSV 한계/NNG 있/VV 결국/NNG 묵인/NNG 하/XSV 듯/NNB 하/XSA 태도/NNG 보이/VV 되/VV\n",
      "퇴임/NNG 이후/NNG 민간/NNG 자원/NNG 적극/NNG 활용/NNG 하/XSV 영리/NNG 기구/NNG 카터/NNP 재단/NNG 설립/NNG 하/XSV 뒤/NNG 민주주의/NNG 실현/NNG 위하/VV 세계/NNG 선거/NNG 감시/NNG 활동/NNG 및/MAG 기니/NNG 벌레/NNG 의하/VV 드라쿤쿠르스/NNP 질병/NNG 방재/NNG 위하/VV 힘쓰/VV\n",
      "미국/NNP 빈곤/NNG 지원/NNG 활동/NNG 사랑/NNG 집짓기/NNG 운동/NNG 국제/NNG 분쟁/NNG 중재/NNG 등/NNB 활동/NNG 하/VV\n",
      "카터/NNP 카터/NNP 행정부/NNG 이후/NNG 미국/NNP 북핵/NNG 위기/NNG 코소보/NNP 전쟁/NNG 이라크/NNP 전쟁/NNG 같이/MAG 미국/NNP 군사/NNG 행동/NNG 최후/NNG 선택/NNG 하/XSV 전통/NNG 사고/NNG 버리/VV 군사/NNG 행동/NNG 선행/NNG 하/XSV 행위/NNG 대하/VV 깊/VA 유감/NNG 표시/NNG 하/XSV 미국/NNP 군사/NNG 활동/NNG 강/VA 하/VV 반대/NNG 입장/NNG 보이/VV 있/VX\n",
      "특히/MAG 국제/NNG 분쟁/NNG 조정/NNG 위하/VV 북한/NNP 김일성/NNP 아이티/NNG 세드/NNG 라스/NNP 장군/NNG 팔레인/NNP 스타인/NNG 하마스/NNP 보스니아/NNP 세르비아/NNP 정권/NNG 미국/NNP 정부/NNG 대하/VV 협상/NNG 거부/NNG 하/XSV 사태/NNG 위기/NNG 초래/NNG 하/XSV 인물/NNG 및/MAG 단체/NNG 직접/MAG 만나/VV 분쟁/NNG 원인/NNG 근본/NNG 해결/NNG 하/XSV 위하/VV 힘쓰/VV\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 30):\n",
    "    print(pos_tagged_corpus[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2er8eeAZdWEI"
   },
   "source": [
    "## 4. Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "umEzIIT_u1L7"
   },
   "source": [
    "동사를 원형으로 복원하도록 하겠습니다.\n",
    "규칙은 다음과 같습니다.\n",
    "\n",
    "1. NNG|NNP|NNB + XSV|XSA --> NNG|NNP|NNB + XSV|XSA + 다\n",
    "2. NNG|NNP|NNB + XSA + VX --> NNG|NNP + XSA + 다\n",
    "3. VV --> VV + 다\n",
    "4. VX --> VX + 다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "orvp9Jk71ind"
   },
   "outputs": [],
   "source": [
    "p1 = re.compile('[가-힣A-Za-z0-9]+/NN. [가-힣A-Za-z0-9]+/XS.')\n",
    "p2 = re.compile('[가-힣A-Za-z0-9]+/NN. [가-힣A-Za-z0-9]+/XSA [가-힣A-Za-z0-9]+/VX')\n",
    "p3 = re.compile('[가-힣A-Za-z0-9]+/VV')\n",
    "p4 = re.compile('[가-힣A-Za-z0-9]+/VX')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mQtv-RQDAGtF"
   },
   "outputs": [],
   "source": [
    "def stemming_text(text):\n",
    "    corpus = []\n",
    "    for sent in text:\n",
    "        ori_sent = sent\n",
    "        mached_terms = re.findall(p1, ori_sent)\n",
    "        for terms in mached_terms:\n",
    "            ori_terms = terms\n",
    "            modi_terms = ''\n",
    "            for term in terms.split(' '):\n",
    "                lemma = term.split('/')[0]\n",
    "                tag = term.split('/')[-1]\n",
    "                modi_terms += lemma\n",
    "            modi_terms += '다/VV'\n",
    "            ori_sent = ori_sent.replace(ori_terms, modi_terms)\n",
    "        \n",
    "        mached_terms = re.findall(p2, ori_sent)\n",
    "        for terms in mached_terms:\n",
    "            ori_terms = terms\n",
    "            modi_terms = ''\n",
    "            for term in terms.split(' '):\n",
    "                lemma = term.split('/')[0]\n",
    "                tag = term.split('/')[-1]\n",
    "                if tag != 'VX':\n",
    "                    modi_terms += lemma\n",
    "            modi_terms += '다/VV'\n",
    "            ori_sent = ori_sent.replace(ori_terms, modi_terms)\n",
    "\n",
    "        mached_terms = re.findall(p3, ori_sent)\n",
    "        for terms in mached_terms:\n",
    "            ori_terms = terms\n",
    "            modi_terms = ''\n",
    "            for term in terms.split(' '):\n",
    "                lemma = term.split('/')[0]\n",
    "                tag = term.split('/')[-1]\n",
    "                modi_terms += lemma\n",
    "            if '다' != modi_terms[-1]:\n",
    "                modi_terms += '다'\n",
    "            modi_terms += '/VV'\n",
    "            ori_sent = ori_sent.replace(ori_terms, modi_terms)\n",
    "\n",
    "        mached_terms = re.findall(p4, ori_sent)\n",
    "        for terms in mached_terms:\n",
    "            ori_terms = terms\n",
    "            modi_terms = ''\n",
    "            for term in terms.split(' '):\n",
    "                lemma = term.split('/')[0]\n",
    "                tag = term.split('/')[-1]\n",
    "                modi_terms += lemma\n",
    "            if '다' != modi_terms[-1]:\n",
    "                modi_terms += '다'\n",
    "            modi_terms += '/VV'\n",
    "            ori_sent = ori_sent.replace(ori_terms, modi_terms)\n",
    "        corpus.append(ori_sent)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B0kAgk2XFfxS"
   },
   "outputs": [],
   "source": [
    "stemming_corpus = stemming_text(pos_tagged_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 565
    },
    "id": "zXhhD9FKGw2u",
    "outputId": "fd1138dc-c046-4125-87f6-a283013cbb93"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "제임스/NNP 얼/NNG 지/NNP 미/NNG 카터/NNP 주니/NNG 어/NNP 민주당/NNP 출신/NNG 미국/NNP 번/NNB 대통령/NNG\n",
      "지미/NNP 카터/NNP 조지아주/NNP 섬터/NNG 카운/NNG 티/NNP 플레인스/NNG 마을/NNG 태어나다/VV\n",
      "조지아/NNP 공과/NNG 대학교/NNG 졸업하다/VV\n",
      "후/NNG 해군/NNG 들어가다/VV 전함/NNG 원자력/NNG 잠수하다/VV 승무원/NNG 일하다/VV\n",
      "년/NNB 미국/NNP 해군/NNG 대위/NNG 예편하다/VV 이후/NNG 땅콩/NNG 면화/NNG 등/NNB 가꾸다/VV 많/VA 돈/NNG 벌다/VV\n",
      "별명/NNG 땅콩/NNG 농부/NNG 알리다/VV 지다/VV\n",
      "년/NNB 조지아/NNP 주/NNP 상원/NNG 의원/NNG 선거/NNG 낙선하다/VV 선거/NNG 부정/NNG 선거/NNG 입증하다/VV 되다/VV 당선되다/VV 년/NNB 조지아/NNP 주/NNG 지사/NNG 선거/NNG 낙선하다/VV 년/NNB 조지아/NNP 주/NNG 지사/NNG 역임하다/VV\n",
      "대통령/NNG 되다/VV 전/NNG 조지아주/NNP 상원/NNG 의원/NNG 번/NNB 연임하다/VV 년/NNB 년/NNB 조지아/NNP 지사/NNG 근무하다/VV\n",
      "조지아/NNP 주지사/NNG 지내다/VV 미국/NNP 살다/VV 흑인/NNG 등용/NNG 법/NNG 내세우다/VV\n",
      "년/NNB 대통령/NNG 선거/NNG 민주당/NNP 후보/NNG 출마하다/VV 도덕주의/NNG 정책/NNG 내세우다/VV 포/NNP 드/NNG 누르다/VV 당선되다/VV\n",
      "카터/NNP 대통령/NNG 에너지/NNG 개발/NNG 촉구하다/VV 공화당/NNP 반대/NNG 무산되다/VV\n",
      "카터/NNP 이집트/NNP 이스라엘/NNP 조정하다/VV 캠프/NNG 데이비드/NNP 안와르/NNP 사다/NNP 트/NNG 대통령/NNG 메나헴/NNP 베기다/VV 수상/NNG 함께/MAG 중동/NNP 평화/NNG 위하다/VV 캠프/NNG 데이비드/NNP 협정/NNG 체결하다/VV\n",
      "그러나/MAJ 공화당/NNP 미국/NNP 유대인/NNG 단체/NNG 반발/NNG 일으키다/VV\n",
      "년/NNB 백악관/NNP 양국/NNG 간/NNB 평화/NNG 조약/NNG 이끌다/VV 지다/VV\n",
      "또한/MAG 소련/NNP 제차/NNG 전략/NNG 무기/NNG 제한/NNG 협상/NNG 조인하다/VV\n",
      "카터/NNP 연대/NNG 후반/NNG 당시/NNG 대한민국/NNP 등/NNB 인권/NNG 후진국/NNG 국민/NNG 인권/NNG 지키다/VV 위하다/VV 노력하다/VV 취임/NNG 이후/NNG 계속하다/VV 도덕/NNG 정치/NNG 내세우다/VV\n",
      "그러나/MAJ 이/NNP 미국/NNP 대사관/NNG 인질/NNG 사건/NNG 인질/NNG 구출/NNG 실패/NNG 이유/NNG 년/NNB 대통령/NNG 선거/NNG 공화당/NNP 로널드/NNP 레이건/NNP 후보/NNG 지다/VV 결국/NNG 재선/NNG 실패하다/VV\n",
      "또한/MAG 임기/NNG 말기/NNG 터지다/VV 소련/NNP 아프가니스탄/NNP 침공/NNG 사건/NNG 인하다/VV 년/NNB 하계/NNG 올림픽/NNG 반공/NNG 국가/NNG 보이콧/NNG 내세우다/VV\n",
      "지미/NNP 카터/NNP 대한민국/NNP 관계/NNG 중요하다/VV 영향/NNG 미치다/VV 대통령/NNG 중/NNB\n",
      "인권/NNG 문제/NNG 주한/NNG 미군/NNG 철수/NNG 문제/NNG 한때/NNG 한미/NNP 관계/NNG 불편하다/VV 하다/VV\n",
      "년/NNB 대한민국/NNP 대하다/VV 북한/NNP 위협/NNG 대비하다/VV 한미연/NNP 합사/NNG 창설하다/VV 년/NNB 단계/NNG 걸치다/VV 주한/NNG 미군/NNG 철수하다/VV 하다/VV\n",
      "그러나/MAJ 주한/NNG 미군/NNG 사령부/NNG 정보기관/NNG 의회/NNG 반대/NNG 부딪히다/VV 주한/NNG 미군/NNG 완전/NNG 철수/NNG 대신/NNG 명/NNG 감축하다/VV 데/NNB 그치다/VV\n",
      "또한/MAG 박정희/NNP 정권/NNG 인권/NNG 문제/NNG 등/NNB 논란/NNG 불협화음/NNG 내다/VV 년/NNB 월/NNB 하순/NNG 대한민국/NNP 방문하다/VV 관계/NNG 다소/MAG 회복되다/VV\n",
      "년/NNB 년/NNB 대한민국/NNP 정치/NNG 격변기/NNG 당시/NNG 대통령/NNG 대하다/VV 하/XSA 태도/NNG 보이다/VV 후/NNG 대한민국/NNP 내/NNB 고조되다/VV 미/NNP 운동/NNG 원인/NNG 되다/VV\n",
      "월/NNG 일/NNG 박정희/NNP 대통령/NNG 김재규/NNP 중앙/NNG 정보/NNG 부장/NNG 의하다/VV 살해되다/VV 것/NNB 대하다/VV 사건/NNG 크/VA 충격/NNG 받다/VV 사/NNP 이러스/NNG 밴스/NNG 국무/NNG 장관/NNG 조문사절/NNG 파견하다/VV\n",
      "군사/NNG 반란과/NNG 쿠데타/NNG 대하다/VV 초기/NNG 강하/VA 비난하다/VV 미국/NNP 정부/NNG 신군부/NNG 설득하다/VV 한계/NNG 있다/VV 결국/NNG 묵인하다/VV 듯하다/VV 태도/NNG 보이다/VV 되다/VV\n",
      "퇴임/NNG 이후/NNG 민간/NNG 자원/NNG 적극/NNG 활용하다/VV 영리/NNG 기구/NNG 카터/NNP 재단/NNG 설립하다/VV 뒤/NNG 민주주의/NNG 실현/NNG 위하다/VV 세계/NNG 선거/NNG 감시/NNG 활동/NNG 및/MAG 기니/NNG 벌레/NNG 의하다/VV 드라쿤쿠르스/NNP 질병/NNG 방재/NNG 위하다/VV 힘쓰다/VV\n",
      "미국/NNP 빈곤/NNG 지원/NNG 활동/NNG 사랑/NNG 집짓기/NNG 운동/NNG 국제/NNG 분쟁/NNG 중재/NNG 등/NNB 활동/NNG 하다/VV\n",
      "카터/NNP 카터/NNP 행정부/NNG 이후/NNG 미국/NNP 북핵/NNG 위기/NNG 코소보/NNP 전쟁/NNG 이라크/NNP 전쟁/NNG 같이/MAG 미국/NNP 군사/NNG 행동/NNG 최후/NNG 선택하다/VV 전통/NNG 사고/NNG 버리다/VV 군사/NNG 행동/NNG 선행하다/VV 행위/NNG 대하다/VV 깊/VA 유감/NNG 표시하다/VV 미국/NNP 군사/NNG 활동/NNG 강/VA 하다/VV 반대/NNG 입장/NNG 보이다/VV 있다/VV\n",
      "특히/MAG 국제/NNG 분쟁/NNG 조정/NNG 위하다/VV 북한/NNP 김일성/NNP 아이티/NNG 세드/NNG 라스/NNP 장군/NNG 팔레인/NNP 스타인/NNG 하마스/NNP 보스니아/NNP 세르비아/NNP 정권/NNG 미국/NNP 정부/NNG 대하다/VV 협상/NNG 거부하다/VV 사태/NNG 위기/NNG 초래하다/VV 인물/NNG 및/MAG 단체/NNG 직접/MAG 만나다/VV 분쟁/NNG 원인/NNG 근본/NNG 해결하다/VV 위하다/VV 힘쓰다/VV\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 30):\n",
    "    print(stemming_corpus[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t0p2DQ8RQCi9"
   },
   "source": [
    "## 5. Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oti-xpR2QJ5P"
   },
   "source": [
    "불용어는 도메인에 맞춰서 다양하게 구축될 수 있습니다.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4u2dbq7UdMtF"
   },
   "outputs": [],
   "source": [
    "stopwords = ['데/NNB', '좀/MAG', '수/NNB', '등/NNB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EmT76QMqS_e_"
   },
   "outputs": [],
   "source": [
    "def remove_stopword_text(text):\n",
    "    corpus = []\n",
    "    for sent in text:\n",
    "        modi_sent = []\n",
    "        for word in sent.split(' '):\n",
    "            if word not in stopwords:\n",
    "                modi_sent.append(word)\n",
    "        corpus.append(' '.join(modi_sent))\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K5F1nxq-TrAi"
   },
   "outputs": [],
   "source": [
    "removed_stopword_corpus = remove_stopword_text(stemming_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 565
    },
    "id": "ZAXz7TjSTzgu",
    "outputId": "6d3a1c06-ca1f-4e81-e6fc-9de300c71dc8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "제임스/NNP 얼/NNG 지/NNP 미/NNG 카터/NNP 주니/NNG 어/NNP 민주당/NNP 출신/NNG 미국/NNP 번/NNB 대통령/NNG\n",
      "지미/NNP 카터/NNP 조지아주/NNP 섬터/NNG 카운/NNG 티/NNP 플레인스/NNG 마을/NNG 태어나다/VV\n",
      "조지아/NNP 공과/NNG 대학교/NNG 졸업하다/VV\n",
      "후/NNG 해군/NNG 들어가다/VV 전함/NNG 원자력/NNG 잠수하다/VV 승무원/NNG 일하다/VV\n",
      "년/NNB 미국/NNP 해군/NNG 대위/NNG 예편하다/VV 이후/NNG 땅콩/NNG 면화/NNG 가꾸다/VV 많/VA 돈/NNG 벌다/VV\n",
      "별명/NNG 땅콩/NNG 농부/NNG 알리다/VV 지다/VV\n",
      "년/NNB 조지아/NNP 주/NNP 상원/NNG 의원/NNG 선거/NNG 낙선하다/VV 선거/NNG 부정/NNG 선거/NNG 입증하다/VV 되다/VV 당선되다/VV 년/NNB 조지아/NNP 주/NNG 지사/NNG 선거/NNG 낙선하다/VV 년/NNB 조지아/NNP 주/NNG 지사/NNG 역임하다/VV\n",
      "대통령/NNG 되다/VV 전/NNG 조지아주/NNP 상원/NNG 의원/NNG 번/NNB 연임하다/VV 년/NNB 년/NNB 조지아/NNP 지사/NNG 근무하다/VV\n",
      "조지아/NNP 주지사/NNG 지내다/VV 미국/NNP 살다/VV 흑인/NNG 등용/NNG 법/NNG 내세우다/VV\n",
      "년/NNB 대통령/NNG 선거/NNG 민주당/NNP 후보/NNG 출마하다/VV 도덕주의/NNG 정책/NNG 내세우다/VV 포/NNP 드/NNG 누르다/VV 당선되다/VV\n",
      "카터/NNP 대통령/NNG 에너지/NNG 개발/NNG 촉구하다/VV 공화당/NNP 반대/NNG 무산되다/VV\n",
      "카터/NNP 이집트/NNP 이스라엘/NNP 조정하다/VV 캠프/NNG 데이비드/NNP 안와르/NNP 사다/NNP 트/NNG 대통령/NNG 메나헴/NNP 베기다/VV 수상/NNG 함께/MAG 중동/NNP 평화/NNG 위하다/VV 캠프/NNG 데이비드/NNP 협정/NNG 체결하다/VV\n",
      "그러나/MAJ 공화당/NNP 미국/NNP 유대인/NNG 단체/NNG 반발/NNG 일으키다/VV\n",
      "년/NNB 백악관/NNP 양국/NNG 간/NNB 평화/NNG 조약/NNG 이끌다/VV 지다/VV\n",
      "또한/MAG 소련/NNP 제차/NNG 전략/NNG 무기/NNG 제한/NNG 협상/NNG 조인하다/VV\n",
      "카터/NNP 연대/NNG 후반/NNG 당시/NNG 대한민국/NNP 인권/NNG 후진국/NNG 국민/NNG 인권/NNG 지키다/VV 위하다/VV 노력하다/VV 취임/NNG 이후/NNG 계속하다/VV 도덕/NNG 정치/NNG 내세우다/VV\n",
      "그러나/MAJ 이/NNP 미국/NNP 대사관/NNG 인질/NNG 사건/NNG 인질/NNG 구출/NNG 실패/NNG 이유/NNG 년/NNB 대통령/NNG 선거/NNG 공화당/NNP 로널드/NNP 레이건/NNP 후보/NNG 지다/VV 결국/NNG 재선/NNG 실패하다/VV\n",
      "또한/MAG 임기/NNG 말기/NNG 터지다/VV 소련/NNP 아프가니스탄/NNP 침공/NNG 사건/NNG 인하다/VV 년/NNB 하계/NNG 올림픽/NNG 반공/NNG 국가/NNG 보이콧/NNG 내세우다/VV\n",
      "지미/NNP 카터/NNP 대한민국/NNP 관계/NNG 중요하다/VV 영향/NNG 미치다/VV 대통령/NNG 중/NNB\n",
      "인권/NNG 문제/NNG 주한/NNG 미군/NNG 철수/NNG 문제/NNG 한때/NNG 한미/NNP 관계/NNG 불편하다/VV 하다/VV\n",
      "년/NNB 대한민국/NNP 대하다/VV 북한/NNP 위협/NNG 대비하다/VV 한미연/NNP 합사/NNG 창설하다/VV 년/NNB 단계/NNG 걸치다/VV 주한/NNG 미군/NNG 철수하다/VV 하다/VV\n",
      "그러나/MAJ 주한/NNG 미군/NNG 사령부/NNG 정보기관/NNG 의회/NNG 반대/NNG 부딪히다/VV 주한/NNG 미군/NNG 완전/NNG 철수/NNG 대신/NNG 명/NNG 감축하다/VV 그치다/VV\n",
      "또한/MAG 박정희/NNP 정권/NNG 인권/NNG 문제/NNG 논란/NNG 불협화음/NNG 내다/VV 년/NNB 월/NNB 하순/NNG 대한민국/NNP 방문하다/VV 관계/NNG 다소/MAG 회복되다/VV\n",
      "년/NNB 년/NNB 대한민국/NNP 정치/NNG 격변기/NNG 당시/NNG 대통령/NNG 대하다/VV 하/XSA 태도/NNG 보이다/VV 후/NNG 대한민국/NNP 내/NNB 고조되다/VV 미/NNP 운동/NNG 원인/NNG 되다/VV\n",
      "월/NNG 일/NNG 박정희/NNP 대통령/NNG 김재규/NNP 중앙/NNG 정보/NNG 부장/NNG 의하다/VV 살해되다/VV 것/NNB 대하다/VV 사건/NNG 크/VA 충격/NNG 받다/VV 사/NNP 이러스/NNG 밴스/NNG 국무/NNG 장관/NNG 조문사절/NNG 파견하다/VV\n",
      "군사/NNG 반란과/NNG 쿠데타/NNG 대하다/VV 초기/NNG 강하/VA 비난하다/VV 미국/NNP 정부/NNG 신군부/NNG 설득하다/VV 한계/NNG 있다/VV 결국/NNG 묵인하다/VV 듯하다/VV 태도/NNG 보이다/VV 되다/VV\n",
      "퇴임/NNG 이후/NNG 민간/NNG 자원/NNG 적극/NNG 활용하다/VV 영리/NNG 기구/NNG 카터/NNP 재단/NNG 설립하다/VV 뒤/NNG 민주주의/NNG 실현/NNG 위하다/VV 세계/NNG 선거/NNG 감시/NNG 활동/NNG 및/MAG 기니/NNG 벌레/NNG 의하다/VV 드라쿤쿠르스/NNP 질병/NNG 방재/NNG 위하다/VV 힘쓰다/VV\n",
      "미국/NNP 빈곤/NNG 지원/NNG 활동/NNG 사랑/NNG 집짓기/NNG 운동/NNG 국제/NNG 분쟁/NNG 중재/NNG 활동/NNG 하다/VV\n",
      "카터/NNP 카터/NNP 행정부/NNG 이후/NNG 미국/NNP 북핵/NNG 위기/NNG 코소보/NNP 전쟁/NNG 이라크/NNP 전쟁/NNG 같이/MAG 미국/NNP 군사/NNG 행동/NNG 최후/NNG 선택하다/VV 전통/NNG 사고/NNG 버리다/VV 군사/NNG 행동/NNG 선행하다/VV 행위/NNG 대하다/VV 깊/VA 유감/NNG 표시하다/VV 미국/NNP 군사/NNG 활동/NNG 강/VA 하다/VV 반대/NNG 입장/NNG 보이다/VV 있다/VV\n",
      "특히/MAG 국제/NNG 분쟁/NNG 조정/NNG 위하다/VV 북한/NNP 김일성/NNP 아이티/NNG 세드/NNG 라스/NNP 장군/NNG 팔레인/NNP 스타인/NNG 하마스/NNP 보스니아/NNP 세르비아/NNP 정권/NNG 미국/NNP 정부/NNG 대하다/VV 협상/NNG 거부하다/VV 사태/NNG 위기/NNG 초래하다/VV 인물/NNG 및/MAG 단체/NNG 직접/MAG 만나다/VV 분쟁/NNG 원인/NNG 근본/NNG 해결하다/VV 위하다/VV 힘쓰다/VV\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 30):\n",
    "    print(removed_stopword_corpus[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9GpD1x2eT0p_"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "한국어_전처리.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
