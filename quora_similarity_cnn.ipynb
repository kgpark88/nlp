{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "quora_similarity_cnn.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uN0Afh5_lqYg"
      },
      "source": [
        "# Quora Questions Pairs 유사도 - CNN\n",
        "- 데이터 전처리 : quora_preprocessing.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUvhgTA0l4Dz"
      },
      "source": [
        "## 구글 드라이브 마운트"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1gM8R4qhlKuZ",
        "outputId": "f81f58fe-a3e6-475a-be0b-3181f894bb6e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Ynw5W0yl83-"
      },
      "source": [
        "## 라이브러리 임포트"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYNCg0txl8b_"
      },
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "from sklearn.model_selection  import train_test_split"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9R7h1MWrx_F"
      },
      "source": [
        "## 학습 데이터 로드"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFZ4psDWunAG"
      },
      "source": [
        "DATA_PATH = '/content/drive/MyDrive/nlpdata/quora/'\n",
        "TEST_SPLIT = 0.1\n",
        "RNG_SEED = 13371447\n",
        "\n",
        "EPOCH=1\n",
        "BATCH_SIZE=1024\n",
        "\n",
        "MAX_SEQUENCE_LENGTH = 31\n",
        "\n",
        "WORD_EMBEDDING_DIM = 100\n",
        "CONV_FEATURE_DIM = 300\n",
        "CONV_OUTPUT_DIM = 128\n",
        "CONV_WINDOW_SIZE = 3\n",
        "SIMILARITY_DENSE_FEATURE_DIM = 200\n",
        "\n",
        "prepro_configs = None\n",
        "\n",
        "with open(DATA_PATH + 'data_configs.json', 'r') as f:\n",
        "    prepro_configs = json.load(f)\n",
        "    \n",
        "VOCAB_SIZE = prepro_configs['vocab_size']"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6j4tiqnzl6vC"
      },
      "source": [
        "train_q1_data = np.load(open(DATA_PATH + 'train_q1.npy', 'rb'))\n",
        "train_q2_data = np.load(open(DATA_PATH + 'train_q2.npy', 'rb'))\n",
        "\n",
        "X = np.stack((train_q1_data, train_q2_data), axis=1) \n",
        "y = np.load(open(DATA_PATH + 'train_label.npy', 'rb'))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XBkDrMonRJX"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cu4kGP76mgoX"
      },
      "source": [
        "## 훈련 셋과 평가 셋 나누기¶"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yJDHGpXmeYR"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmLYxLT5vMe_"
      },
      "source": [
        "train_Q1 = X_train[:,0]\n",
        "train_Q2 = X_train[:,1]\n",
        "test_Q1 = X_test[:,0]\n",
        "test_Q2 = X_test[:,1]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WdWtcStcveoQ"
      },
      "source": [
        "def rearrange(base, hypothesis, label):\n",
        "    features = {\"x1\": base, \"x2\": hypothesis}\n",
        "    return features, label\n",
        "\n",
        "def train_input_fn():\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((train_Q1, train_Q2, train_y))\n",
        "    dataset = dataset.shuffle(buffer_size=10000)\n",
        "    dataset = dataset.batch(BATCH_SIZE)\n",
        "    dataset = dataset.map(rearrange)\n",
        "    dataset = dataset.repeat(EPOCH)\n",
        "    iterator = dataset.make_one_shot_iterator()\n",
        "    \n",
        "    return iterator.get_next()\n",
        "\n",
        "def eval_input_fn():\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((eval_Q1, eval_Q2, eval_y))\n",
        "    dataset = dataset.shuffle(buffer_size=10000)\n",
        "    dataset = dataset.batch(BATCH_SIZE)\n",
        "    dataset = dataset.map(rearrange)\n",
        "    iterator = dataset.make_one_shot_iterator()\n",
        "    \n",
        "    return iterator.get_next()"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uLrEVzkvjZI"
      },
      "source": [
        "## 모델 셋업"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlQ7VOQKvUg3"
      },
      "source": [
        "def basic_conv_sementic_network(inputs, name):\n",
        "    conv_layer = tf.keras.layers.Conv1D(CONV_FEATURE_DIM, \n",
        "                                        CONV_WINDOW_SIZE, \n",
        "                                        activation=tf.nn.relu, \n",
        "                                        name=name + 'conv_1d',\n",
        "                                        padding='same')(inputs)\n",
        "    \n",
        "    max_pool_layer = tf.keras.layers.MaxPool1D(MAX_SEQUENCE_LENGTH, \n",
        "                                               1)(conv_layer)\n",
        "\n",
        "    output_layer = tf.keras.layers.Dense(CONV_OUTPUT_DIM, \n",
        "                                         activation=tf.nn.relu,\n",
        "                                         name=name + 'dense')(max_pool_layer)\n",
        "    output_layer = tf.squeeze(output_layer, 1)\n",
        "    \n",
        "    return output_layer"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWUNkQMJvnTn"
      },
      "source": [
        "def model_fn(features, labels, mode):\n",
        "    TRAIN = mode == tf.estimator.ModeKeys.TRAIN\n",
        "    EVAL = mode == tf.estimator.ModeKeys.EVAL\n",
        "    PREDICT = mode == tf.estimator.ModeKeys.PREDICT\n",
        "    \n",
        "    embedding = tf.keras.layers.Embedding(VOCAB_SIZE,\n",
        "                                          WORD_EMBEDDING_DIM)\n",
        "    \n",
        "    base_embedded_matrix = embedding(features['x1'])\n",
        "    hypothesis_embedded_matrix = embedding(features['x2'])\n",
        "    \n",
        "    base_embedded_matrix = tf.keras.layers.Dropout(0.2)(base_embedded_matrix)\n",
        "    hypothesis_embedded_matrix = tf.keras.layers.Dropout(0.2)(hypothesis_embedded_matrix)  \n",
        "    \n",
        "    base_sementic_matrix = basic_conv_sementic_network(base_embedded_matrix, 'base')\n",
        "    hypothesis_sementic_matrix = basic_conv_sementic_network(hypothesis_embedded_matrix, 'hypothesis')  \n",
        "    \n",
        "    merged_matrix = tf.concat([base_sementic_matrix, hypothesis_sementic_matrix], -1)\n",
        "\n",
        "    similarity_dense_layer = tf.keras.layers.Dense(SIMILARITY_DENSE_FEATURE_DIM,\n",
        "                                             activation=tf.nn.relu)(merged_matrix)\n",
        "    \n",
        "    similarity_dense_layer = tf.keras.layers.Dropout(0.2)(similarity_dense_layer)    \n",
        "    logit_layer = tf.keras.layers.Dense(1)(similarity_dense_layer)\n",
        "    logit_layer = tf.squeeze(logit_layer, 1)\n",
        "    similarity = tf.nn.sigmoid(logit_layer)\n",
        "    \n",
        "    if PREDICT:\n",
        "        return tf.estimator.EstimatorSpec(\n",
        "                  mode=mode,\n",
        "                  predictions={\n",
        "                      'is_duplicate':similarity\n",
        "                  })\n",
        "    \n",
        "    loss = tf.losses.sigmoid_cross_entropy(labels, logit_layer)\n",
        "\n",
        "    if EVAL:\n",
        "        accuracy = tf.metrics.accuracy(labels, tf.round(similarity))\n",
        "        return tf.estimator.EstimatorSpec(\n",
        "                  mode=mode,\n",
        "                  eval_metric_ops= {'acc': accuracy},\n",
        "                  loss=loss)\n",
        "    \n",
        "    if TRAIN:\n",
        "        global_step = tf.train.get_global_step()\n",
        "        train_op = tf.train.AdamOptimizer(1e-3).minimize(loss, global_step)\n",
        "\n",
        "        return tf.estimator.EstimatorSpec(\n",
        "                  mode=mode,\n",
        "                  train_op=train_op,\n",
        "                  loss=loss)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nQxabYTmoBf"
      },
      "source": [
        "## 모델 훈련 & 평가"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JheJHyxQmjwo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "794a5622-5efe-4c6a-8e2a-095f304d301f"
      },
      "source": [
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"6\" \n",
        "\n",
        "model_dir = os.path.join(os.getcwd(), DATA_PATH + \"checkpoint/cnn/\")\n",
        "os.makedirs(model_dir, exist_ok=True)\n",
        "\n",
        "est = tf.estimator.Estimator(model_fn, model_dir=model_dir)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_model_dir': '/content/drive/MyDrive/nlpdata/quora/checkpoint/cnn/', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "id": "F-GW6bXArVC7",
        "outputId": "49316c75-e52a-4f88-a2b2-6e9b4c6b05cc"
      },
      "source": [
        "est.train(train_input_fn) #train"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-6ae2a9f15bd0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_input_fn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_fn, hooks, steps, max_steps, saving_listeners)\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m       \u001b[0msaving_listeners\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_listeners_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m       \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss for final step: %s.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m   1173\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model_distributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1174\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1175\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_train_model_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model_default\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m   1199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1200\u001b[0m       features, labels, input_hooks = (\n\u001b[0;32m-> 1201\u001b[0;31m           self._get_features_and_labels_from_input_fn(input_fn, ModeKeys.TRAIN))\n\u001b[0m\u001b[1;32m   1202\u001b[0m       \u001b[0mworker_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_hooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1203\u001b[0m       estimator_spec = self._call_model_fn(features, labels, ModeKeys.TRAIN,\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_get_features_and_labels_from_input_fn\u001b[0;34m(self, input_fn, mode)\u001b[0m\n\u001b[1;32m   1035\u001b[0m     \u001b[0;34m\"\"\"Extracts the `features` and labels from return values of `input_fn`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m     return estimator_util.parse_input_fn_result(\n\u001b[0;32m-> 1037\u001b[0;31m         self._call_input_fn(input_fn, mode))\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extract_batch_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds_evaluated\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_call_input_fn\u001b[0;34m(self, input_fn, mode, input_context)\u001b[0m\n\u001b[1;32m   1128\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_context'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1129\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/cpu:0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1130\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_model_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-596e7eae99f3>\u001b[0m in \u001b[0;36mtrain_input_fn\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_input_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_Q1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_Q2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_y' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7dBeQZ4v2Am"
      },
      "source": [
        "est.evaluate(eval_input_fn) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmGkblH8shWL"
      },
      "source": [
        "## Load test dataset & create submit dataset to kaggle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7g33EyNKskK1"
      },
      "source": [
        "test_q1_data = np.load(open(DATA_PATH + 'test_q1.npy', 'rb'))\n",
        "test_q2_data = np.load(open(DATA_PATH + 'test_q2.npy', 'rb'))\n",
        "test_id_data = np.load(open(DATA_PATH + 'test_id.npy', 'rb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQjIhjfhwH2W"
      },
      "source": [
        "predict_input_fn = tf.estimator.inputs.numpy_input_fn(x={\"x1\":test_q1_data,\n",
        "                                                         \"x2\":test_q2_data},\n",
        "                                                      shuffle=False)\n",
        "\n",
        "predictions = np.array([p['is_duplicate'] for p in est.predict(input_fn=predict_input_fn)])\n",
        "\n",
        "output = pd.DataFrame( data={\"test_id\":test_id_data, \"is_duplicate\": list(predictions)} )\n",
        "output.to_csv(\"quora_cnn.csv\", index=False, quoting=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZrCehiitQTA"
      },
      "source": [
        "## kgggle에 결과 제출 및 스코어 확인\n",
        "- https://www.kaggle.com/c/quora-question-pairs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGcufy0_s-Ya"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}