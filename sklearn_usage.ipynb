{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sklearn_usage.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.7"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIF1zRgq8UHc"
      },
      "source": [
        "# Scikit-Learn 문서 특징 추출 기능\n",
        "\n",
        "- DictVectorizer: 각 단어의 수를 세어놓은 사전에서 BoW(Bag-of-Words) 인코딩 벡터를 만든다.\n",
        "\n",
        "- CountVectorizer: 문서 집합에서 단어 토큰을 생성하고 각 단어의 수를 세어 BoW 인코딩 벡터를 만든다.\n",
        "\n",
        "- TfidfVectorizer: CountVectorizer와 비슷하지만 TF-IDF 방식으로 단어의 가중치를 조정한 BoW 인코딩 벡터를 만든다.\n",
        "\n",
        "- HashingVectorizer: 해시 함수(hash function)을 사용하여 적은 메모리와 빠른 속도로 BoW 인코딩 벡터를 만든다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCLOVR2P8ixA"
      },
      "source": [
        "## DictVectorizer\n",
        "- 단어의 카운트(출현 빈도, frequency) 나타내는 Dictionary 정보를 입력받아 벡터화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLVdO_ce8SJP"
      },
      "source": [
        "from sklearn.feature_extraction import DictVectorizer\n",
        "\n",
        "v = DictVectorizer(sparse=False)\n",
        "D = [{'foo': 1, 'bar': 2}, {'foo': 3, 'baz': 1}]\n",
        "X = v.fit_transform(D)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GDQuVhAw8yr8",
        "outputId": "2aa30865-e3cd-40a8-f6ac-fcb1f3094ebb"
      },
      "source": [
        "print(X)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[2. 0. 1.]\n",
            " [0. 1. 3.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OIU-VIXS8sGn",
        "outputId": "b7a02df2-2156-4974-b5ec-3ea8bd542849"
      },
      "source": [
        "print(v.feature_names_)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['bar', 'baz', 'foo']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p9NhuwMa4m4T",
        "outputId": "3ee7fb96-4723-46d8-a93b-fabf9b23bd33"
      },
      "source": [
        "v.inverse_transform(X)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'bar': 2.0, 'foo': 1.0}, {'baz': 1.0, 'foo': 3.0}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxrXH5xW80rv",
        "outputId": "02a7da48-32e1-4ea8-af8b-3517cf2343b5"
      },
      "source": [
        "v.transform({'foo': 4, 'unseen_feature': 3})"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 4.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOVEw4S79KOR"
      },
      "source": [
        "## CountVectorizer\n",
        "- 단어들의 카운트(출현 빈도, frequency)로 여러 문서들을 벡터화\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tX6KadPS82jW"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "corpus = [\n",
        "    'This is the first document.',\n",
        "    'This is the second second document.',\n",
        "    'And the third one.',\n",
        "    'Is this the first document?',\n",
        "    'The last document?',\n",
        "]\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(corpus)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8EbV9rnf6cOx",
        "outputId": "765050b3-970e-49a6-ecaa-4aab0f151e3d"
      },
      "source": [
        "print(vectorizer.get_feature_names())"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['and', 'document', 'first', 'is', 'last', 'one', 'second', 'the', 'third', 'this']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bqncQ65B9h7f",
        "outputId": "7ddd519f-23c1-4951-ecdd-7af6519a491c"
      },
      "source": [
        "print(X.toarray())"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 1 1 1 0 0 0 1 0 1]\n",
            " [0 1 0 1 0 0 2 1 0 1]\n",
            " [1 0 0 0 0 1 0 1 1 0]\n",
            " [0 1 1 1 0 0 0 1 0 1]\n",
            " [0 1 0 0 1 0 0 1 0 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6biDcu6b9oJv",
        "outputId": "8675c977-7fb8-450e-df1b-55d2817cfc53"
      },
      "source": [
        "print(X.toarray())"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 1 1 1 0 0 0 1 0 1]\n",
            " [0 1 0 1 0 0 2 1 0 1]\n",
            " [1 0 0 0 0 1 0 1 1 0]\n",
            " [0 1 1 1 0 0 0 1 0 1]\n",
            " [0 1 0 0 1 0 0 1 0 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5Q3zjV8983k"
      },
      "source": [
        "### Stop Words\n",
        "- Stop Words 는 문서에서 단어장을 생성할 때 무시할 수 있는 단어를 말한다. \n",
        "- 보통 영어의 관사나 접속사, 한국어의 조사 등이 여기에 해당한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mqXM69no9uQe",
        "outputId": "d24ca592-ebfc-4f4c-9ac4-c684fac96af3"
      },
      "source": [
        "vectorizer = CountVectorizer(stop_words=[\"and\", \"is\", \"the\", \"this\"]).fit(corpus)\n",
        "print(vectorizer.vocabulary_)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'first': 1, 'document': 0, 'second': 4, 'third': 5, 'one': 3, 'last': 2}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-oWY2wm3-DSe",
        "outputId": "6cdcfcbd-348f-40f9-9670-03413a63de8d"
      },
      "source": [
        "vectorizer = CountVectorizer(stop_words=\"english\").fit(corpus)\n",
        "print(vectorizer.vocabulary_)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'document': 0, 'second': 1}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSydl-MY-ggs"
      },
      "source": [
        "### 토큰\n",
        "- analyzer, tokenizer, token_pattern 등의 인수로 사용할 토큰 생성기를 선택할 수 있다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cB-FLtcy-Gxu",
        "outputId": "c6b68640-0ce8-453f-9159-cf5178876a6b"
      },
      "source": [
        "vectorizer = CountVectorizer(analyzer=\"char\").fit(corpus)\n",
        "print(vectorizer.vocabulary_)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'t': 16, 'h': 8, 'i': 9, 's': 15, ' ': 0, 'e': 6, 'f': 7, 'r': 14, 'd': 5, 'o': 13, 'c': 4, 'u': 17, 'm': 11, 'n': 12, '.': 1, 'a': 3, '?': 2, 'l': 10}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CdGrjoqF-mPf",
        "outputId": "b8fb7103-a352-4857-cdcc-5842dcb52a9f"
      },
      "source": [
        "vectorizer = CountVectorizer(token_pattern=\"t\\w+\").fit(corpus)\n",
        "print(vectorizer.vocabulary_)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'this': 2, 'the': 0, 'third': 1}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0kupBvH-one",
        "outputId": "ee57ee2b-6397-4701-ec95-1cf4076ccfc8"
      },
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "vectorizer = CountVectorizer(tokenizer=nltk.word_tokenize).fit(corpus)\n",
        "print(vectorizer.vocabulary_)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "{'this': 11, 'is': 5, 'the': 9, 'first': 4, 'document': 3, '.': 0, 'second': 8, 'and': 2, 'third': 10, 'one': 7, '?': 1, 'last': 6}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TexZdhd-48S"
      },
      "source": [
        "### N그램\n",
        "- N그램은 단어장 생성에 사용할 토큰의 크기를 결정한다. \n",
        "- 모노그램(monogram)은 토큰 하나만 단어로 사용한다. \n",
        "- 바이그램(bigram)은 두 개의 연결된 토큰을 하나의 단어로 사용한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4C_zQTrz-sQW",
        "outputId": "070f5b26-b709-45b9-b7d9-d45e173716aa"
      },
      "source": [
        "vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n",
        "X2 = vectorizer2.fit_transform(corpus)\n",
        "print(vectorizer2.get_feature_names())"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['and the', 'first document', 'is the', 'is this', 'last document', 'second document', 'second second', 'the first', 'the last', 'the second', 'the third', 'third one', 'this is', 'this the']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_25eAq_l_Igy"
      },
      "source": [
        "### 빈도수\n",
        "- max_df, min_df 인수를 사용하여 문서에서 토큰이 나타난 횟수를 기준으로 단어장을 구성할 수도 있다. \n",
        "- 토큰의 빈도가 max_df로 지정한 값을 초과 하거나 min_df로 지정한 값보다 작은 경우에는 무시한다. \n",
        "- 인수 값은 정수인 경우 횟수, 부동소수점인 경우 비중을 뜻한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RYAS8K35_BPH",
        "outputId": "055f05ea-298f-46f9-9845-07c2279810fd"
      },
      "source": [
        "vectorizer = CountVectorizer(max_df=4, min_df=2).fit(corpus)\n",
        "print(vectorizer.vocabulary_)\n",
        "print(vectorizer.stop_words_)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'this': 3, 'is': 2, 'first': 1, 'document': 0}\n",
            "{'last', 'third', 'second', 'and', 'the', 'one'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N_XzUZ1Z_Het",
        "outputId": "a02b03cc-b82e-44ab-fdf0-d70e6c8fe3af"
      },
      "source": [
        "vectorizer.transform(corpus).toarray().sum(axis=0)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([4, 2, 3, 3])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VymhVBx_ZM2"
      },
      "source": [
        "## TF-IDF(Term Frequency – Inverse Document Frequency)\n",
        "- 단어의 빈도와 역 문서 빈도를 사용하여 DTM 내의 각 단어들마다 중요한 정도를 가중치로 주는 방법\n",
        "- 문서의 유사도를 구하는 작업, 검색 시스템에서 검색 결과의 중요도를 정하는 작업, 문서 내에서 특정 단어의 중요도를 구하는 작업 등에 사용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RrKVl_7Z_V0M",
        "outputId": "d26d444e-1ce2-4d91-cdee-9cd0470a3492"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "corpus = [\n",
        "    'This is the first document.',\n",
        "    'This is the second second document.',\n",
        "    'And the third one.',\n",
        "    'Is this the first document?'\n",
        "]\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "\n",
        "print(vectorizer.get_feature_names())"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7_6lQ5LAJAe",
        "outputId": "48dea20f-88b6-4ba6-e085-f0885358e078"
      },
      "source": [
        "print(X.shape)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4, 9)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_fInp8VH_Kaa",
        "outputId": "587dc05d-75af-4d4c-85b6-122d4b0de43d"
      },
      "source": [
        "print(X.toarray())"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.         0.43877674 0.54197657 0.43877674 0.         0.\n",
            "  0.35872874 0.         0.43877674]\n",
            " [0.         0.27230147 0.         0.27230147 0.         0.85322574\n",
            "  0.22262429 0.         0.27230147]\n",
            " [0.55280532 0.         0.         0.         0.55280532 0.\n",
            "  0.28847675 0.55280532 0.        ]\n",
            " [0.         0.43877674 0.54197657 0.43877674 0.         0.\n",
            "  0.35872874 0.         0.43877674]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zx54dJsGBT-g"
      },
      "source": [
        "## HashingVectorizer\n",
        "- 해시 함수(hash function)을 사용하여 적은 메모리와 빠른 속도로 BoW 인코딩 벡터를만든다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jl3B6jRF_Q7R"
      },
      "source": [
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "\n",
        "corpus = [\n",
        "    'This is the first document.',\n",
        "    'This is the second second document.',\n",
        "    'And the third one.',\n",
        "    'Is this the first document?'\n",
        "]\n",
        "\n",
        "vectorizer = HashingVectorizer(n_features=2**4)\n",
        "X = vectorizer.fit_transform(corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NxdH_O_YLRFh",
        "outputId": "c1b95f68-c7f1-45e6-c03b-93e63699d31b"
      },
      "source": [
        "print(X.shape)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4, 16)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQM8uW1dBgFy",
        "outputId": "53ed41f4-272c-4322-cfdc-c666e78b72c7"
      },
      "source": [
        "print(X.toarray())"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-0.57735027  0.          0.          0.          0.          0.\n",
            "   0.          0.         -0.57735027  0.          0.          0.\n",
            "   0.          0.57735027  0.          0.        ]\n",
            " [-0.40824829  0.          0.          0.          0.          0.\n",
            "   0.          0.          0.          0.          0.          0.81649658\n",
            "   0.          0.40824829  0.          0.        ]\n",
            " [ 0.          0.          0.          0.         -0.5         0.5\n",
            "   0.          0.          0.          0.          0.          0.\n",
            "   0.         -0.5        -0.5         0.        ]\n",
            " [-0.57735027  0.          0.          0.          0.          0.\n",
            "   0.          0.         -0.57735027  0.          0.          0.\n",
            "   0.          0.57735027  0.          0.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rma4FrFhBhZA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}