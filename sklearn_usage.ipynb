{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eIF1zRgq8UHc"
   },
   "source": [
    "# Scikit-Learn 문서 특징 추출 기능\n",
    "\n",
    "- DictVectorizer: 각 단어의 수를 세어놓은 사전에서 BoW(Bag-of-Words) 인코딩 벡터를 만든다.\n",
    "\n",
    "- CountVectorizer: 문서 집합에서 단어 토큰을 생성하고 각 단어의 수를 세어 BoW 인코딩 벡터를 만든다.\n",
    "\n",
    "- TfidfVectorizer: CountVectorizer와 비슷하지만 TF-IDF 방식으로 단어의 가중치를 조정한 BoW 인코딩 벡터를 만든다.\n",
    "\n",
    "- HashingVectorizer: 해시 함수(hash function)을 사용하여 적은 메모리와 빠른 속도로 BoW 인코딩 벡터를 만든다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HCLOVR2P8ixA"
   },
   "source": [
    "## DictVectorizer\n",
    "- 단어의 카운트(출현 빈도, frequency) 나타내는 Dictionary 정보를 입력받아 벡터화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "DLVdO_ce8SJP"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "v = DictVectorizer(sparse=False)\n",
    "D = [{'foo': 1, 'bar': 2}, {'foo': 3, 'baz': 1}]\n",
    "X = v.fit_transform(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GDQuVhAw8yr8",
    "outputId": "58a2ada1-b804-4334-a088-8d454fc80a4a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2. 0. 1.]\n",
      " [0. 1. 3.]]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OIU-VIXS8sGn",
    "outputId": "bcf808aa-b98d-40ca-f826-73b998802318"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bar', 'baz', 'foo']\n"
     ]
    }
   ],
   "source": [
    "print(v.feature_names_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p9NhuwMa4m4T",
    "outputId": "64d14aa9-398b-4bab-c444-2ef0e1453561"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'bar': 2.0, 'foo': 1.0}, {'baz': 1.0, 'foo': 3.0}]"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.inverse_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hxrXH5xW80rv",
    "outputId": "27ed4cd4-11b9-4c9a-c02f-de205c2e49e1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 4.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.transform({'foo': 4, 'unseen_feature': 3})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LOVEw4S79KOR"
   },
   "source": [
    "## CountVectorizer\n",
    "- 단어들의 카운트(출현 빈도, frequency)로 여러 문서들을 벡터화\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tX6KadPS82jW",
    "outputId": "282088b7-b155-4e17-df2b-a8d56f73a1c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'document', 'first', 'is', 'last', 'one', 'second', 'the', 'third', 'this']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This is the second second document.',\n",
    "    'And the third one.',\n",
    "    'Is this the first document?',\n",
    "    'The last document?',\n",
    "]\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8EbV9rnf6cOx",
    "outputId": "cb7729e3-0d82-4964-b13d-81f8bc1a916c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'document', 'first', 'is', 'last', 'one', 'second', 'the', 'third', 'this']\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bqncQ65B9h7f",
    "outputId": "37c81c73-e9e0-4d4b-ed07-dc33eb59cbe4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 1 0 0 0 1 0 1]\n",
      " [0 1 0 1 0 0 2 1 0 1]\n",
      " [1 0 0 0 0 1 0 1 1 0]\n",
      " [0 1 1 1 0 0 0 1 0 1]\n",
      " [0 1 0 0 1 0 0 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6biDcu6b9oJv",
    "outputId": "491ffe5c-db4f-4ea2-ddc3-0b1f844fb871"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 0 0 0 0 1 0 0 0 0 1 0]\n",
      " [0 0 1 0 0 1 1 0 0 1 0 0 1 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 1 1 0 0]\n",
      " [0 1 0 1 0 0 0 1 0 0 0 0 0 1]\n",
      " [0 0 0 0 1 0 0 0 1 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(X2.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O5Q3zjV8983k"
   },
   "source": [
    "### Stop Words\n",
    "- Stop Words 는 문서에서 단어장을 생성할 때 무시할 수 있는 단어를 말한다. \n",
    "- 보통 영어의 관사나 접속사, 한국어의 조사 등이 여기에 해당한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mqXM69no9uQe",
    "outputId": "ebb6c44e-0f1b-4bef-82d7-47ef0fff66ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'first': 1, 'document': 0, 'second': 4, 'third': 5, 'one': 3, 'last': 2}\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(stop_words=[\"and\", \"is\", \"the\", \"this\"]).fit(corpus)\n",
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-oWY2wm3-DSe",
    "outputId": "1734c944-d604-4326-aaeb-7d540f366332"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'document': 0, 'second': 1}\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(stop_words=\"english\").fit(corpus)\n",
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TSydl-MY-ggs"
   },
   "source": [
    "### 토큰\n",
    "- analyzer, tokenizer, token_pattern 등의 인수로 사용할 토큰 생성기를 선택할 수 있다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cB-FLtcy-Gxu",
    "outputId": "f3e669ed-a155-4e35-94ca-883134a69c0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'t': 16, 'h': 8, 'i': 9, 's': 15, ' ': 0, 'e': 6, 'f': 7, 'r': 14, 'd': 5, 'o': 13, 'c': 4, 'u': 17, 'm': 11, 'n': 12, '.': 1, 'a': 3, '?': 2, 'l': 10}\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(analyzer=\"char\").fit(corpus)\n",
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CdGrjoqF-mPf",
    "outputId": "81bb37ec-66dc-46ff-b48f-a96382ef29f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'this': 2, 'the': 0, 'third': 1}\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(token_pattern=\"t\\w+\").fit(corpus)\n",
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y0kupBvH-one",
    "outputId": "a8189612-7c84-44d3-c2ab-e8f626dd6a30"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "{'this': 11, 'is': 5, 'the': 9, 'first': 4, 'document': 3, '.': 0, 'second': 8, 'and': 2, 'third': 10, 'one': 7, '?': 1, 'last': 6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "vectorizer = CountVectorizer(tokenizer=nltk.word_tokenize).fit(corpus)\n",
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6TexZdhd-48S"
   },
   "source": [
    "### N그램\n",
    "- N그램은 단어장 생성에 사용할 토큰의 크기를 결정한다. \n",
    "- 모노그램(monogram)은 토큰 하나만 단어로 사용한다. \n",
    "- 바이그램(bigram)은 두 개의 연결된 토큰을 하나의 단어로 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4C_zQTrz-sQW",
    "outputId": "717d2e91-3ac2-4641-fe75-7ca30f62f147"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and the', 'first document', 'is the', 'is this', 'last document', 'second document', 'second second', 'the first', 'the last', 'the second', 'the third', 'third one', 'this is', 'this the']\n"
     ]
    }
   ],
   "source": [
    "vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n",
    "X2 = vectorizer2.fit_transform(corpus)\n",
    "print(vectorizer2.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_25eAq_l_Igy"
   },
   "source": [
    "### 빈도수\n",
    "- max_df, min_df 인수를 사용하여 문서에서 토큰이 나타난 횟수를 기준으로 단어장을 구성할 수도 있다. \n",
    "- 토큰의 빈도가 max_df로 지정한 값을 초과 하거나 min_df로 지정한 값보다 작은 경우에는 무시한다. \n",
    "- 인수 값은 정수인 경우 횟수, 부동소수점인 경우 비중을 뜻한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RYAS8K35_BPH",
    "outputId": "9aa6ea3f-8ba5-4555-807d-41a582ee23d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'this': 3, 'is': 2, 'first': 1, 'document': 0}\n",
      "{'and', 'the', 'one', 'last', 'third', 'second'}\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(max_df=4, min_df=2).fit(corpus)\n",
    "print(vectorizer.vocabulary_)\n",
    "print(vectorizer.stop_words_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N_XzUZ1Z_Het",
    "outputId": "ee70e47b-a2f0-49c4-b553-bd45b1a242c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 2, 3, 3])"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.transform(corpus).toarray().sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8VymhVBx_ZM2"
   },
   "source": [
    "## TF-IDF(Term Frequency – Inverse Document Frequency)\n",
    "- 단어의 빈도와 역 문서 빈도를 사용하여 DTM 내의 각 단어들마다 중요한 정도를 가중치로 주는 방법\n",
    "- 문서의 유사도를 구하는 작업, 검색 시스템에서 검색 결과의 중요도를 정하는 작업, 문서 내에서 특정 단어의 중요도를 구하는 작업 등에 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RrKVl_7Z_V0M",
    "outputId": "0d54e34a-f578-40a1-a052-a7f055dd8ec1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This is the second second document.',\n",
    "    'And the third one.',\n",
    "    'Is this the first document?'\n",
    "]\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f7_6lQ5LAJAe",
    "outputId": "169a3a46-4c2a-4c68-c5fe-e4d5e75dd5ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 9)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_fInp8VH_Kaa",
    "outputId": "e6f4f848-4e1e-432b-eb3c-c040bad23f46"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.43877674 0.54197657 0.43877674 0.         0.\n",
      "  0.35872874 0.         0.43877674]\n",
      " [0.         0.27230147 0.         0.27230147 0.         0.85322574\n",
      "  0.22262429 0.         0.27230147]\n",
      " [0.55280532 0.         0.         0.         0.55280532 0.\n",
      "  0.28847675 0.55280532 0.        ]\n",
      " [0.         0.43877674 0.54197657 0.43877674 0.         0.\n",
      "  0.35872874 0.         0.43877674]]\n"
     ]
    }
   ],
   "source": [
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zx54dJsGBT-g"
   },
   "source": [
    "## HashingVectorizer\n",
    "- 해시 함수(hash function)을 사용하여 적은 메모리와 빠른 속도로 BoW 인코딩 벡터를만든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jl3B6jRF_Q7R",
    "outputId": "9683dc76-4334-429d-fa37-5a2398f16a5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 16)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This is the second second document.',\n",
    "    'And the third one.',\n",
    "    'Is this the first document?'\n",
    "]\n",
    "\n",
    "vectorizer = HashingVectorizer(n_features=2**4)\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PQM8uW1dBgFy",
    "outputId": "4cd4bfcb-6e76-45e7-f221-b82e606febb9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.57735027  0.          0.          0.          0.          0.\n",
      "   0.          0.         -0.57735027  0.          0.          0.\n",
      "   0.          0.57735027  0.          0.        ]\n",
      " [-0.40824829  0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.81649658\n",
      "   0.          0.40824829  0.          0.        ]\n",
      " [ 0.          0.          0.          0.         -0.5         0.5\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.         -0.5        -0.5         0.        ]\n",
      " [-0.57735027  0.          0.          0.          0.          0.\n",
      "   0.          0.         -0.57735027  0.          0.          0.\n",
      "   0.          0.57735027  0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rma4FrFhBhZA"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "sklearn_usage.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
